{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thisnetwork test out the use of spatial transform network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "import  torchgeometry.core as tgm\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "random.seed(1008)\n",
    "random.shuffle(labeled_scene_index)\n",
    "train_idx = labeled_scene_index[:22]\n",
    "val_idx = labeled_scene_index[22:26]\n",
    "test_idx = labeled_scene_index[26:]\n",
    "\n",
    "transform = transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "train_set = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_idx,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "val_set = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_idx,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "\n",
    "test_set = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_idx,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True, num_workers=2, \n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = BATCH_SIZE, \n",
    "                                         shuffle = True, num_workers=2, \n",
    "                                         collate_fn = collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = BATCH_SIZE, \n",
    "                                          shuffle = True, num_workers=2, \n",
    "                                          collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_concat_features(feature_maps):\n",
    "    '''\n",
    "    feature maps be in shape B V C H W\n",
    "    returns in shape B C H W\n",
    "    '''\n",
    "    first_row = torch.cat([feature_maps[:, i] for i in range(3)], dim=3)\n",
    "    second_row = torch.cat([feature_maps[:, i] for i in range(3,6)], dim=3)\n",
    "    result = torch.cat([first_row, second_row], dim=2)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VPN_model import PPMBilinear, _SameDecoder, _DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexTransformModule(nn.Module):\n",
    "    def __init__(self, num_view=6):\n",
    "        '''\n",
    "        Takes in input B, V, C, H, W\n",
    "        '''\n",
    "        super(ComplexTransformModule, self).__init__()\n",
    "        \n",
    "        self.num_view = num_view\n",
    "\n",
    "        self.mat_list = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_view):\n",
    "            self.mat_list += [SpatialTransformer(1024, 3)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Takes in B,V,C,H, W, perform warpping on each image and concatenate by position\n",
    "        '''\n",
    "        B, V, C, H, W = x.size()\n",
    "        view_comb = self.mat_list[0](x[:, 0])\n",
    "        for i in range(1, V):\n",
    "            view_comb += self.mat_list[i](x[:, i])\n",
    "            # for each view, perform the warpped view\n",
    "            #x[:, i] = self.mat_list[i](x[:, i])\n",
    "        #Concatenate the view\n",
    "        # x = position_concat_features(x)\n",
    "        return view_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_encoder1 = torchvision.models.resnet50(pretrained = False)\n",
    "resnet_encoder1 = list(resnet_encoder1.children())[:-3]\n",
    "resnet_encoder1 = nn.Sequential(*resnet_encoder1)\n",
    "for param in resnet_encoder1.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = PPMBilinear(fc_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, V, C, H, W = encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "view_comb = mat_list[0](encoded[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 20])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "view_comb2 = mat_list[1](encoded[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 20])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_comb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 20])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(view_comb + view_comb2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_list = nn.ModuleList()\n",
    "for i in range(6):\n",
    "    mat_list+=[SpatialTransformer(1024, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,V,C,H,W = encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "encoded[:, 0] = mat_list[0](encoded[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size):\n",
    "        '''\n",
    "        Takes input in Bx 1024 x 16 x 20\n",
    "        '''\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "        self._in_ch = in_channels \n",
    "        self._ksize = kernel_size\n",
    "\n",
    "        self.prep_warper = nn.Sequential(*[\n",
    "            nn.Conv2d(self._in_ch, 32, kernel_size=self._ksize, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=self._ksize, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "            \n",
    "        ])\n",
    "    \n",
    "        self.warper_generator = nn.Sequential(*[\n",
    "                    nn.Linear(32*8*10, 1024), \n",
    "                    nn.ReLU(inplace = True),\n",
    "                    nn.Linear(1024, 9),\n",
    "                    nn.Tanh()\n",
    "        ])\n",
    "\n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "        Forward pass of the STN module. \n",
    "        x -> input feature map \n",
    "        x should be the feature map for a single view\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        #localization net\n",
    "        homo_mat = self.prep_warper(x)\n",
    "        # concatenate 3 dim\n",
    "        homo_mat = homo_mat.view(B, -1)\n",
    "        \n",
    "        homo_mat = self.warper_generator(homo_mat) # BV 3 X3 \n",
    "        #reshape to homo matrix\n",
    "        homo_mat = homo_mat.view(-1, 3, 3)\n",
    "        # grid sample on original view\n",
    "        warper = tgm.HomographyWarper(H, W)\n",
    "        warpped = warper(x, homo_mat)\n",
    "        return warpped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 20])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded.view([1,6,1024, 16, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_view = encoded[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = curr_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_warper = nn.Sequential(*[\n",
    "            nn.Conv2d(1024, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "            \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 10])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_mat = prep_warper(curr_view)\n",
    "homo_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_mat = homo_mat.view(B, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2560])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "warper_generator = nn.Sequential(*[\n",
    "                    nn.Linear(32*8*10, 1024), \n",
    "                    nn.ReLU(inplace = True),\n",
    "                    nn.Linear(1024, 9),\n",
    "                    nn.Tanh()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_mat = warper_generator(homo_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_mat = homo_mat.view(-1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "warper = tgm.HomographyWarper(H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "warpped = warper(curr_view, homo_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1024, 16, 20])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = ComplexTransformModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "transformed = tm(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16, 20])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = vpn_model_v2(resnet_encoder1, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _, roadmpa, extra = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =torch.stack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "test_output = temp_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 800, 800])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vpn_model_v2(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(vpn_model_v2, self).__init__()\n",
    "        self.num_views = 6\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.transform = ComplexTransformModule()\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self, x, return_feat = False):\n",
    "        # flatten the output along channel: C x (HW)\n",
    "        # weights are not shared, i.e. each first view input has\n",
    "        # own VRM to get its top down view feature map \n",
    "        # i here in range 6(MN, N=6,M=1(MODALITY))\n",
    "        # j here in range num_channels\n",
    "        # \n",
    "        B,V,C,H,W = x.shape\n",
    "        x = x.view(B*V, C, H, W)\n",
    "        x = self.encoder(x)\n",
    "        # return to B V \n",
    "        x = x.view([B,V] + list(x.size()[1:]))\n",
    "        \n",
    "        x =  self.transform(x) # B x c x h x w\n",
    "        \n",
    "        x = self.decoder([x])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
