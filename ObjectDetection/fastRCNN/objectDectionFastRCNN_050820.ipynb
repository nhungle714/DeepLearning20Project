{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "LMpEF8ZDd-lt",
    "outputId": "d4563cbf-c762-48a9-dbb7-6cb0ecc0c4fd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "os.chdir('/scratch/nhl256/dl_project/code/')\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import *\n",
    "from nhung_data_helper import FastRCNNLabeledDataset\n",
    "from helper import *\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGxs-4igvAzk"
   },
   "source": [
    "### 05/08/20\n",
    "\n",
    "- Concat 6 images along 1 dim before passing through the model\n",
    "- Saving the feature extractor and the fasterRCNN model\n",
    "- ResNet18 for feature extraction using the weights from self-supervised task ('/scratch/nhl256/dl_project/model/pretrain_colorization_resnet18.pth')\n",
    "- optimizer = torch.optim.Adam(params, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "X9PMrKKreF3N",
    "outputId": "38430281-d6fc-4ab0-d551-3da75203b517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "\n",
    "# image_folder = '../data'\n",
    "# annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "image_folder = 'data/data'\n",
    "annotation_csv = 'data/data/annotation.csv'\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# image_folder = '/Users/nhungle/Downloads/dl20_data'\n",
    "# annotation_csv = '/Users/nhungle/Downloads/dl20_data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vmrTQbev_exi",
    "outputId": "eb026039-9b09-452f-e116-efaa4274ee56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htb4imOktWfj"
   },
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "random.seed(1008)\n",
    "random.shuffle(labeled_scene_index)\n",
    "train_labeled_scene_index = labeled_scene_index[:22]\n",
    "val_labeled_scene_index = labeled_scene_index[22:26]\n",
    "test_labeled_scene_index = labeled_scene_index[26:]\n",
    "BATCH_SIZE=1\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_valset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "val_loader = torch.utils.data.DataLoader(fasterRCNN_valset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gDRi-fS1TAlH",
    "outputId": "724493de-d954-425f-c0e9-09abdcaf4960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(color_encoder, self).__init__()\n",
    "        encoder = torchvision.models.resnet18(pretrained=False)\n",
    "        encoder = list(encoder.children())[1:-3]\n",
    "        encoder = [nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)] + encoder\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x \n",
    "    \n",
    "class _SameDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, num_layers, out_activation = 'relu'):\n",
    "        super(_SameDecoder, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        layers += [\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) \n",
    "        ]*(num_layers-2)\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if out_activation == 'relu' else nn.Sigmoid(),\n",
    "        ]\n",
    "        self.decode = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "    \n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()\n",
    "\n",
    "class color_decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(color_decoder, self).__init__()\n",
    "        self.dec1 = _SameDecoder(256, 512,kernel_size=2, stride=2, num_layers=4)\n",
    "        self.dec2 = _SameDecoder(512, 256,kernel_size=2, stride=2, num_layers=4)\n",
    "        self.dec3 = _SameDecoder(256, 128,kernel_size=2, stride=2, num_layers=2)\n",
    "        self.dec4 = _SameDecoder(128, 64,kernel_size=2, stride=2, num_layers=2)\n",
    "        self.conv_out = nn.Conv2d(64, 2, 3 , padding=1)\n",
    "        \n",
    "        self.final_upsample = nn.Upsample((256, 306), mode='bilinear', align_corners=False)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.dec1(x)\n",
    "        x= self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        x = self.dec4(x)\n",
    "        x = self.conv_out(x)\n",
    "        #print('after layers the size is', x.size())\n",
    "        x = self.final_upsample(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class color_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(color_model, self).__init__()\n",
    "        self.encoder = color_encoder()\n",
    "        self.decoder = color_decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mid = self.encoder(x)\n",
    "        output = self.decoder(mid)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_checkpoint = torch.load('/scratch/nhl256/dl_project/model/pretrain_colorization_resnet18.pth')\n",
    "pretrain_model = color_model()\n",
    "pretrain_model.load_state_dict(color_checkpoint['best_model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLuS_ttrTT_j"
   },
   "source": [
    "### Get Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_encoder = pretrain_model.encoder.encoder\n",
    "pretrain_encoder = list(pretrain_encoder.children())[1:]\n",
    "feature_extractor = [nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)] + pretrain_encoder\n",
    "feature_extractor = nn.Sequential(*feature_extractor)\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVI7nFEITWEw"
   },
   "outputs": [],
   "source": [
    "def concat_features(features, dim = 2):\n",
    "    #dim 0 ==> stacking the images in the channel dimension\n",
    "    #dim 1 ==> stacking the images in row dimension\n",
    "    #dim 2 ==> stacking the images in column dimension\n",
    "    tensor_tuples = torch.unbind(features, dim=0)\n",
    "    concatenated_fm = torch.cat(tensor_tuples, dim=dim)\n",
    "    return concatenated_fm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTHbwweW-4B"
   },
   "source": [
    "### Inspect if it works on a pretrained FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HwKa9R3TXInF",
    "outputId": "35be123d-30e1-4db2-cedc-53da22551da2"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features) # torch.Size([256, 16, 120])\n",
    "    features = features.view(3, 256, 40*16)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "CnnOF2RsT1j_",
    "outputId": "0eee6720-3222-45fe-def7-f737ad7976c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(17.8849, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_box_reg': tensor(0.2243, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(1.8392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(3.3491, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=False)\n",
    "model = model.to(device)\n",
    "output1 = model(images, targets)\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF4a0pmZYUwr"
   },
   "source": [
    "## Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fH3zlobZUog"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcDAxVX7YQ31"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=False)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 1\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r7B7MC6YQ2I"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(feature_extractor, model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    feature_extractor.train()\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for sample, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.to(device)\n",
    "        batchsize = sample.shape[0]\n",
    "        fe_batch = []\n",
    "        for i in range(batchsize):\n",
    "            image_tensor = sample[i]\n",
    "            features = feature_extractor(image_tensor)\n",
    "            #print(features.shape)\n",
    "            features = concat_features(features)\n",
    "            features = features.view(3, 256, 40*16)\n",
    "            #print(features.shape)\n",
    "            fe_batch.append(features)\n",
    "\n",
    "        images = list(image.to(device) for image in fe_batch)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        #print(losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "DWvOavWsYQ0H",
    "outputId": "258b60d7-fd62-4a18-c86e-1f6cfd8d1aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/252]  eta: 0:05:50  lr: 0.000000  loss: 20.4135 (20.4135)  loss_classifier: 15.3485 (15.3485)  loss_box_reg: 0.0171 (0.0171)  loss_objectness: 0.8709 (0.8709)  loss_rpn_box_reg: 4.1771 (4.1771)  time: 1.3890  data: 0.7828  max mem: 3011\n",
      "Epoch: [0]  [ 20/252]  eta: 0:01:03  lr: 0.000008  loss: 4.1465 (7.8224)  loss_classifier: 0.3130 (3.7392)  loss_box_reg: 0.0375 (0.0368)  loss_objectness: 0.5850 (0.6020)  loss_rpn_box_reg: 3.1668 (3.4444)  time: 0.2167  data: 0.0043  max mem: 3450\n",
      "Epoch: [0]  [ 40/252]  eta: 0:00:51  lr: 0.000016  loss: 1.4301 (4.7714)  loss_classifier: 0.1098 (1.9840)  loss_box_reg: 0.0143 (0.0292)  loss_objectness: 0.2926 (0.4689)  loss_rpn_box_reg: 1.0413 (2.2893)  time: 0.2149  data: 0.0038  max mem: 3450\n",
      "Epoch: [0]  [ 60/252]  eta: 0:00:45  lr: 0.000024  loss: 1.1898 (3.5747)  loss_classifier: 0.1113 (1.3739)  loss_box_reg: 0.0052 (0.0217)  loss_objectness: 0.1867 (0.3788)  loss_rpn_box_reg: 0.8219 (1.8003)  time: 0.2159  data: 0.0041  max mem: 3450\n",
      "Epoch: [0]  [ 80/252]  eta: 0:00:39  lr: 0.000032  loss: 1.0408 (2.9329)  loss_classifier: 0.0912 (1.0585)  loss_box_reg: 0.0032 (0.0178)  loss_objectness: 0.1221 (0.3158)  loss_rpn_box_reg: 0.7652 (1.5408)  time: 0.2160  data: 0.0041  max mem: 3450\n",
      "Epoch: [0]  [100/252]  eta: 0:00:35  lr: 0.000040  loss: 1.1380 (2.5776)  loss_classifier: 0.1225 (0.8702)  loss_box_reg: 0.0042 (0.0155)  loss_objectness: 0.1104 (0.2778)  loss_rpn_box_reg: 0.8767 (1.4141)  time: 0.2462  data: 0.0344  max mem: 3450\n",
      "Epoch: [0]  [120/252]  eta: 0:00:32  lr: 0.000048  loss: 1.0235 (2.3134)  loss_classifier: 0.0904 (0.7415)  loss_box_reg: 0.0034 (0.0139)  loss_objectness: 0.1049 (0.2509)  loss_rpn_box_reg: 0.7797 (1.3070)  time: 0.2918  data: 0.0778  max mem: 3450\n",
      "Epoch: [0]  [140/252]  eta: 0:00:26  lr: 0.000056  loss: 1.0344 (2.1275)  loss_classifier: 0.1049 (0.6513)  loss_box_reg: 0.0022 (0.0130)  loss_objectness: 0.0977 (0.2333)  loss_rpn_box_reg: 0.7953 (1.2299)  time: 0.2131  data: 0.0035  max mem: 3450\n",
      "Epoch: [0]  [160/252]  eta: 0:00:21  lr: 0.000064  loss: 0.8702 (1.9811)  loss_classifier: 0.1098 (0.5841)  loss_box_reg: 0.0021 (0.0123)  loss_objectness: 0.0907 (0.2170)  loss_rpn_box_reg: 0.6408 (1.1678)  time: 0.2138  data: 0.0039  max mem: 3450\n",
      "Epoch: [0]  [180/252]  eta: 0:00:16  lr: 0.000072  loss: 0.9642 (1.8677)  loss_classifier: 0.0946 (0.5307)  loss_box_reg: 0.0016 (0.0117)  loss_objectness: 0.0924 (0.2030)  loss_rpn_box_reg: 0.6119 (1.1223)  time: 0.2146  data: 0.0039  max mem: 3450\n",
      "Epoch: [0]  [200/252]  eta: 0:00:12  lr: 0.000080  loss: 0.9696 (1.7750)  loss_classifier: 0.1328 (0.4907)  loss_box_reg: 0.0085 (0.0118)  loss_objectness: 0.0663 (0.1901)  loss_rpn_box_reg: 0.6919 (1.0824)  time: 0.2159  data: 0.0040  max mem: 3450\n",
      "Epoch: [0]  [220/252]  eta: 0:00:07  lr: 0.000088  loss: 0.9027 (1.6982)  loss_classifier: 0.1000 (0.4554)  loss_box_reg: 0.0056 (0.0116)  loss_objectness: 0.0533 (0.1789)  loss_rpn_box_reg: 0.6964 (1.0522)  time: 0.2163  data: 0.0041  max mem: 3450\n",
      "Epoch: [0]  [240/252]  eta: 0:00:02  lr: 0.000096  loss: 0.6960 (1.6261)  loss_classifier: 0.0492 (0.4237)  loss_box_reg: 0.0016 (0.0111)  loss_objectness: 0.0509 (0.1686)  loss_rpn_box_reg: 0.5892 (1.0227)  time: 0.2164  data: 0.0042  max mem: 3450\n",
      "Epoch: [0]  [251/252]  eta: 0:00:00  lr: 0.000100  loss: 0.6243 (1.5872)  loss_classifier: 0.0521 (0.4104)  loss_box_reg: 0.0029 (0.0113)  loss_objectness: 0.0442 (0.1633)  loss_rpn_box_reg: 0.4902 (1.0021)  time: 0.2166  data: 0.0041  max mem: 3450\n",
      "Epoch: [0] Total time: 0:00:57 (0.2289 s / it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8828, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(feature_extractor, model, optimizer, test_loader, device, epoch, print_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NnX1worb7BY"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features)\n",
    "    features = features.view(3, 256, 40*16)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "oZH8AvMGePCh",
    "outputId": "e1e991fc-bfc1-4704-cd11-1b15c60c9e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGrIApq0cR_2"
   },
   "outputs": [],
   "source": [
    "# model_test = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model_test = model_test.to(device)\n",
    "# model_test.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# x = list(image.to(device) for image in x)\n",
    "# predictions = model_test(x)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuHvpn_iaQpL"
   },
   "source": [
    "## Evaluate One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpX1eYADap3P"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(val_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFij7vtUYQx6"
   },
   "outputs": [],
   "source": [
    "def reorder_coord(pred_bboxes):\n",
    "    xmin, ymin, xmax, ymax = pred_bboxes.unbind(1)\n",
    "    return torch.stack((xmax, xmax, xmin, xmin, ymax, ymin, ymax, ymin), dim=1).view(-1, 2, 4)\n",
    "\n",
    "def get_bounding_boxes(samples):\n",
    "    # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n",
    "    # You need to return a tuple with size 'batch_size' and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of object\n",
    "\n",
    "    #Preparing inputs\n",
    "    batchsize = samples.shape[0]\n",
    "    fe_batch = []\n",
    "    for i in range(batchsize):\n",
    "        image_tensor = sample[i]\n",
    "        features = feature_extractor(image_tensor)\n",
    "        #print(features.shape)\n",
    "        features = concat_features(features)\n",
    "        features = features.view(3, 256, 40*16)\n",
    "        #print(features.shape)\n",
    "        fe_batch.append(features)\n",
    "\n",
    "    images = list(image.to(device) for image in fe_batch)\n",
    "    predictions = model(images)\n",
    "    res = []\n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i]\n",
    "        pred_bboxes = prediction['boxes']\n",
    "        reorder_pred_bboxes = reorder_coord(pred_bboxes)\n",
    "        res.append(reorder_pred_bboxes)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfZQ_U-waTAI"
   },
   "outputs": [],
   "source": [
    "def eval_one_epoch(feature_extractor, model, dataloader):\n",
    "    model.eval()\n",
    "    feature_extractor.eval()\n",
    "    total = 0\n",
    "    total_ats_bounding_boxes = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        total += 1\n",
    "        sample, target = data\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.cuda()\n",
    "\n",
    "        predicted_bounding_boxes = get_bounding_boxes(sample)[0].cpu()\n",
    "        \n",
    "\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes,\n",
    "                                                        target[0]['bounding_box'])\n",
    "#         print('Number of pred bboxes {}'.format(predicted_bounding_boxes.shape))\n",
    "#         print('ats_bounding_boxes {}'.format(ats_bounding_boxes))\n",
    "\n",
    "        total_ats_bounding_boxes += ats_bounding_boxes\n",
    "    return total_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMuwreKLaTF6"
   },
   "outputs": [],
   "source": [
    "total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE0B0pQ0e8FO"
   },
   "source": [
    "## Train and Eval for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXio30HjaS9-"
   },
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get feature extractor\n",
    "pretrain_encoder = pretrain_model.encoder.encoder\n",
    "pretrain_encoder = list(pretrain_encoder.children())[1:]\n",
    "feature_extractor = [nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)] + pretrain_encoder\n",
    "feature_extractor = nn.Sequential(*feature_extractor)\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(feature_extractor, model, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                       'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                      }\n",
    "    losses = []\n",
    "    total_ats = []\n",
    "    best_total_ats = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(feature_extractor, model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        lr_scheduler.step()\n",
    "        total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, val_loader)\n",
    "        losses.append(loss)\n",
    "        total_ats.append(total_ats_bounding_boxes)\n",
    "        print('epoch {} loss {} total_ats {}'.format(epoch, loss, total_ats))\n",
    "\n",
    "        if total_ats_bounding_boxes > best_total_ats:\n",
    "            best_total_ats = total_ats_bounding_boxes\n",
    "            best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                               'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                     }\n",
    "            torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet18_0509_epoch{}.pth'.format(epoch))\n",
    "\n",
    "    return losses, total_ats, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2772]  eta: 0:28:52  lr: 0.000000  loss: 13.8289 (13.8289)  loss_classifier: 10.7143 (10.7143)  loss_box_reg: 0.0666 (0.0666)  loss_objectness: 0.6826 (0.6826)  loss_rpn_box_reg: 2.3655 (2.3655)  time: 0.6250  data: 0.2597  max mem: 4431\n",
      "Epoch: [0]  [ 500/2772]  eta: 0:08:12  lr: 0.000050  loss: 0.7623 (1.2079)  loss_classifier: 0.1692 (0.3731)  loss_box_reg: 0.0203 (0.0229)  loss_objectness: 0.0292 (0.1390)  loss_rpn_box_reg: 0.4930 (0.6730)  time: 0.2145  data: 0.0037  max mem: 5190\n",
      "Epoch: [0]  [1000/2772]  eta: 0:06:27  lr: 0.000100  loss: 0.5845 (0.9700)  loss_classifier: 0.1292 (0.2737)  loss_box_reg: 0.0424 (0.0247)  loss_objectness: 0.0171 (0.0823)  loss_rpn_box_reg: 0.3856 (0.5894)  time: 0.2152  data: 0.0038  max mem: 5190\n",
      "Epoch: [0]  [1500/2772]  eta: 0:04:43  lr: 0.000100  loss: 0.6308 (0.8627)  loss_classifier: 0.1192 (0.2223)  loss_box_reg: 0.0307 (0.0262)  loss_objectness: 0.0113 (0.0618)  loss_rpn_box_reg: 0.3887 (0.5523)  time: 0.2630  data: 0.0480  max mem: 5190\n",
      "Epoch: [0]  [2000/2772]  eta: 0:02:50  lr: 0.000100  loss: 0.6173 (0.8169)  loss_classifier: 0.0925 (0.1967)  loss_box_reg: 0.0158 (0.0269)  loss_objectness: 0.0076 (0.0518)  loss_rpn_box_reg: 0.4408 (0.5416)  time: 0.2138  data: 0.0035  max mem: 5196\n",
      "Epoch: [0]  [2500/2772]  eta: 0:00:59  lr: 0.000100  loss: 0.6425 (0.7759)  loss_classifier: 0.1066 (0.1805)  loss_box_reg: 0.0358 (0.0277)  loss_objectness: 0.0238 (0.0459)  loss_rpn_box_reg: 0.4375 (0.5218)  time: 0.2132  data: 0.0036  max mem: 5196\n",
      "Epoch: [0]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5466 (0.7586)  loss_classifier: 0.1011 (0.1738)  loss_box_reg: 0.0310 (0.0278)  loss_objectness: 0.0186 (0.0437)  loss_rpn_box_reg: 0.3286 (0.5133)  time: 0.2149  data: 0.0039  max mem: 5196\n",
      "Epoch: [0] Total time: 0:10:08 (0.2195 s / it)\n",
      "epoch 0 loss 0.5843818187713623 total_ats [tensor(2.4805)]\n",
      "Epoch: [1]  [   0/2772]  eta: 0:24:58  lr: 0.000100  loss: 0.4987 (0.4987)  loss_classifier: 0.0722 (0.0722)  loss_box_reg: 0.0072 (0.0072)  loss_objectness: 0.0058 (0.0058)  loss_rpn_box_reg: 0.4135 (0.4135)  time: 0.5405  data: 0.3131  max mem: 5565\n",
      "Epoch: [1]  [ 500/2772]  eta: 0:08:12  lr: 0.000100  loss: 0.5488 (0.6022)  loss_classifier: 0.0822 (0.1148)  loss_box_reg: 0.0120 (0.0311)  loss_objectness: 0.0135 (0.0198)  loss_rpn_box_reg: 0.4090 (0.4365)  time: 0.2141  data: 0.0036  max mem: 5565\n",
      "Epoch: [1]  [1000/2772]  eta: 0:06:24  lr: 0.000100  loss: 0.5506 (0.5967)  loss_classifier: 0.0864 (0.1103)  loss_box_reg: 0.0320 (0.0292)  loss_objectness: 0.0241 (0.0199)  loss_rpn_box_reg: 0.4006 (0.4373)  time: 0.2145  data: 0.0038  max mem: 5565\n",
      "Epoch: [1]  [1500/2772]  eta: 0:04:35  lr: 0.000100  loss: 0.6108 (0.5959)  loss_classifier: 0.0983 (0.1123)  loss_box_reg: 0.0321 (0.0304)  loss_objectness: 0.0238 (0.0193)  loss_rpn_box_reg: 0.3864 (0.4339)  time: 0.2162  data: 0.0040  max mem: 5565\n",
      "Epoch: [1]  [2000/2772]  eta: 0:02:47  lr: 0.000100  loss: 0.5442 (0.5973)  loss_classifier: 0.0993 (0.1112)  loss_box_reg: 0.0222 (0.0296)  loss_objectness: 0.0186 (0.0189)  loss_rpn_box_reg: 0.3633 (0.4376)  time: 0.2147  data: 0.0037  max mem: 5565\n",
      "Epoch: [1]  [2500/2772]  eta: 0:00:58  lr: 0.000100  loss: 0.5250 (0.5981)  loss_classifier: 0.1048 (0.1106)  loss_box_reg: 0.0198 (0.0290)  loss_objectness: 0.0071 (0.0187)  loss_rpn_box_reg: 0.3919 (0.4398)  time: 0.2163  data: 0.0040  max mem: 5565\n",
      "Epoch: [1]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5609 (0.5971)  loss_classifier: 0.1015 (0.1110)  loss_box_reg: 0.0224 (0.0291)  loss_objectness: 0.0160 (0.0188)  loss_rpn_box_reg: 0.3789 (0.4383)  time: 0.2153  data: 0.0037  max mem: 5565\n",
      "Epoch: [1] Total time: 0:09:59 (0.2161 s / it)\n",
      "epoch 1 loss 0.4178313612937927 total_ats [tensor(2.4805), tensor(2.8058)]\n",
      "Epoch: [2]  [   0/2772]  eta: 0:33:17  lr: 0.000100  loss: 0.4584 (0.4584)  loss_classifier: 0.0442 (0.0442)  loss_box_reg: 0.0192 (0.0192)  loss_objectness: 0.0432 (0.0432)  loss_rpn_box_reg: 0.3517 (0.3517)  time: 0.7206  data: 0.5049  max mem: 5565\n",
      "Epoch: [2]  [ 500/2772]  eta: 0:08:13  lr: 0.000100  loss: 0.5609 (0.5759)  loss_classifier: 0.0998 (0.1111)  loss_box_reg: 0.0252 (0.0298)  loss_objectness: 0.0086 (0.0170)  loss_rpn_box_reg: 0.3957 (0.4180)  time: 0.2161  data: 0.0041  max mem: 5565\n",
      "Epoch: [2]  [1000/2772]  eta: 0:06:23  lr: 0.000100  loss: 0.5139 (0.5680)  loss_classifier: 0.0821 (0.1065)  loss_box_reg: 0.0209 (0.0278)  loss_objectness: 0.0051 (0.0166)  loss_rpn_box_reg: 0.3584 (0.4172)  time: 0.2170  data: 0.0043  max mem: 5565\n",
      "Epoch: [2]  [1500/2772]  eta: 0:04:35  lr: 0.000100  loss: 0.6159 (0.5712)  loss_classifier: 0.1059 (0.1045)  loss_box_reg: 0.0228 (0.0274)  loss_objectness: 0.0085 (0.0174)  loss_rpn_box_reg: 0.4369 (0.4219)  time: 0.2151  data: 0.0039  max mem: 5565\n",
      "Epoch: [2]  [2000/2772]  eta: 0:02:46  lr: 0.000100  loss: 0.5359 (0.5704)  loss_classifier: 0.0917 (0.1042)  loss_box_reg: 0.0267 (0.0275)  loss_objectness: 0.0057 (0.0178)  loss_rpn_box_reg: 0.3884 (0.4209)  time: 0.2152  data: 0.0037  max mem: 5565\n",
      "Epoch: [2]  [2500/2772]  eta: 0:00:58  lr: 0.000100  loss: 0.5801 (0.5728)  loss_classifier: 0.1113 (0.1059)  loss_box_reg: 0.0217 (0.0281)  loss_objectness: 0.0082 (0.0177)  loss_rpn_box_reg: 0.3970 (0.4211)  time: 0.2152  data: 0.0038  max mem: 5565\n",
      "Epoch: [2]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.4989 (0.5756)  loss_classifier: 0.0991 (0.1066)  loss_box_reg: 0.0269 (0.0281)  loss_objectness: 0.0057 (0.0183)  loss_rpn_box_reg: 0.3443 (0.4225)  time: 0.2149  data: 0.0038  max mem: 5565\n",
      "Epoch: [2] Total time: 0:09:59 (0.2162 s / it)\n",
      "epoch 2 loss 1.065444827079773 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230)]\n",
      "Epoch: [3]  [   0/2772]  eta: 0:24:41  lr: 0.000010  loss: 0.4840 (0.4840)  loss_classifier: 0.1408 (0.1408)  loss_box_reg: 0.0430 (0.0430)  loss_objectness: 0.0117 (0.0117)  loss_rpn_box_reg: 0.2886 (0.2886)  time: 0.5344  data: 0.3120  max mem: 5565\n",
      "Epoch: [3]  [ 500/2772]  eta: 0:08:13  lr: 0.000010  loss: 0.5535 (0.5298)  loss_classifier: 0.1055 (0.1045)  loss_box_reg: 0.0218 (0.0290)  loss_objectness: 0.0114 (0.0168)  loss_rpn_box_reg: 0.3988 (0.3795)  time: 0.2161  data: 0.0041  max mem: 5565\n",
      "Epoch: [3]  [1000/2772]  eta: 0:06:24  lr: 0.000010  loss: 0.5247 (0.5289)  loss_classifier: 0.1247 (0.1056)  loss_box_reg: 0.0398 (0.0292)  loss_objectness: 0.0050 (0.0171)  loss_rpn_box_reg: 0.3361 (0.3770)  time: 0.2194  data: 0.0045  max mem: 5565\n",
      "Epoch: [3]  [1500/2772]  eta: 0:04:35  lr: 0.000010  loss: 0.5170 (0.5208)  loss_classifier: 0.1007 (0.1053)  loss_box_reg: 0.0210 (0.0293)  loss_objectness: 0.0179 (0.0168)  loss_rpn_box_reg: 0.3387 (0.3694)  time: 0.2161  data: 0.0041  max mem: 5565\n",
      "Epoch: [3]  [2000/2772]  eta: 0:02:47  lr: 0.000010  loss: 0.5089 (0.5155)  loss_classifier: 0.0973 (0.1053)  loss_box_reg: 0.0231 (0.0293)  loss_objectness: 0.0052 (0.0165)  loss_rpn_box_reg: 0.3297 (0.3643)  time: 0.2163  data: 0.0041  max mem: 5565\n",
      "Epoch: [3]  [2500/2772]  eta: 0:00:59  lr: 0.000010  loss: 0.4401 (0.5163)  loss_classifier: 0.0701 (0.1052)  loss_box_reg: 0.0138 (0.0293)  loss_objectness: 0.0048 (0.0163)  loss_rpn_box_reg: 0.3448 (0.3655)  time: 0.2164  data: 0.0042  max mem: 5565\n",
      "Epoch: [3]  [2771/2772]  eta: 0:00:00  lr: 0.000010  loss: 0.4586 (0.5160)  loss_classifier: 0.0929 (0.1056)  loss_box_reg: 0.0178 (0.0294)  loss_objectness: 0.0058 (0.0163)  loss_rpn_box_reg: 0.3218 (0.3648)  time: 0.2154  data: 0.0040  max mem: 5565\n",
      "Epoch: [3] Total time: 0:10:02 (0.2172 s / it)\n",
      "epoch 3 loss 0.2687547206878662 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569)]\n",
      "Epoch: [4]  [   0/2772]  eta: 0:24:01  lr: 0.000010  loss: 0.4951 (0.4951)  loss_classifier: 0.0727 (0.0727)  loss_box_reg: 0.0181 (0.0181)  loss_objectness: 0.0389 (0.0389)  loss_rpn_box_reg: 0.3655 (0.3655)  time: 0.5202  data: 0.3007  max mem: 5565\n",
      "Epoch: [4]  [ 500/2772]  eta: 0:09:24  lr: 0.000010  loss: 0.4761 (0.4875)  loss_classifier: 0.1023 (0.1010)  loss_box_reg: 0.0329 (0.0284)  loss_objectness: 0.0091 (0.0165)  loss_rpn_box_reg: 0.2886 (0.3416)  time: 0.2182  data: 0.0044  max mem: 5565\n",
      "Epoch: [4]  [1000/2772]  eta: 0:06:52  lr: 0.000010  loss: 0.5107 (0.4926)  loss_classifier: 0.0704 (0.1021)  loss_box_reg: 0.0237 (0.0289)  loss_objectness: 0.0172 (0.0158)  loss_rpn_box_reg: 0.3353 (0.3458)  time: 0.2165  data: 0.0040  max mem: 5565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4]  [1500/2772]  eta: 0:04:49  lr: 0.000010  loss: 0.4108 (0.4873)  loss_classifier: 0.0805 (0.1015)  loss_box_reg: 0.0189 (0.0292)  loss_objectness: 0.0205 (0.0158)  loss_rpn_box_reg: 0.2855 (0.3407)  time: 0.2315  data: 0.0197  max mem: 5565\n",
      "Epoch: [4]  [2000/2772]  eta: 0:02:53  lr: 0.000010  loss: 0.4591 (0.4886)  loss_classifier: 0.0973 (0.1013)  loss_box_reg: 0.0233 (0.0290)  loss_objectness: 0.0225 (0.0156)  loss_rpn_box_reg: 0.2871 (0.3427)  time: 0.2149  data: 0.0040  max mem: 5565\n",
      "Epoch: [4]  [2500/2772]  eta: 0:01:00  lr: 0.000010  loss: 0.4791 (0.4889)  loss_classifier: 0.0940 (0.1018)  loss_box_reg: 0.0248 (0.0292)  loss_objectness: 0.0114 (0.0156)  loss_rpn_box_reg: 0.2899 (0.3423)  time: 0.2164  data: 0.0041  max mem: 5565\n",
      "Epoch: [4]  [2771/2772]  eta: 0:00:00  lr: 0.000010  loss: 0.4309 (0.4894)  loss_classifier: 0.0844 (0.1019)  loss_box_reg: 0.0267 (0.0292)  loss_objectness: 0.0124 (0.0156)  loss_rpn_box_reg: 0.2915 (0.3427)  time: 0.2169  data: 0.0043  max mem: 5565\n",
      "Epoch: [4] Total time: 0:10:16 (0.2225 s / it)\n",
      "epoch 4 loss 0.3613768517971039 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446)]\n",
      "Epoch: [5]  [   0/2772]  eta: 0:19:34  lr: 0.000010  loss: 0.6815 (0.6815)  loss_classifier: 0.1156 (0.1156)  loss_box_reg: 0.0347 (0.0347)  loss_objectness: 0.0139 (0.0139)  loss_rpn_box_reg: 0.5173 (0.5173)  time: 0.4235  data: 0.2067  max mem: 5565\n",
      "Epoch: [5]  [ 500/2772]  eta: 0:08:16  lr: 0.000010  loss: 0.4596 (0.4803)  loss_classifier: 0.0739 (0.1003)  loss_box_reg: 0.0149 (0.0277)  loss_objectness: 0.0049 (0.0152)  loss_rpn_box_reg: 0.3221 (0.3371)  time: 0.2169  data: 0.0043  max mem: 5565\n",
      "Epoch: [5]  [1000/2772]  eta: 0:06:25  lr: 0.000010  loss: 0.3995 (0.4741)  loss_classifier: 0.0870 (0.0999)  loss_box_reg: 0.0180 (0.0293)  loss_objectness: 0.0097 (0.0145)  loss_rpn_box_reg: 0.2559 (0.3304)  time: 0.2163  data: 0.0041  max mem: 5565\n",
      "Epoch: [5]  [1500/2772]  eta: 0:04:36  lr: 0.000010  loss: 0.5488 (0.4768)  loss_classifier: 0.0978 (0.1005)  loss_box_reg: 0.0290 (0.0298)  loss_objectness: 0.0157 (0.0148)  loss_rpn_box_reg: 0.3456 (0.3316)  time: 0.2185  data: 0.0044  max mem: 5565\n",
      "Epoch: [5]  [2000/2772]  eta: 0:02:47  lr: 0.000010  loss: 0.4473 (0.4753)  loss_classifier: 0.0847 (0.1001)  loss_box_reg: 0.0204 (0.0299)  loss_objectness: 0.0054 (0.0149)  loss_rpn_box_reg: 0.2640 (0.3304)  time: 0.2185  data: 0.0047  max mem: 5565\n",
      "Epoch: [5]  [2500/2772]  eta: 0:00:59  lr: 0.000010  loss: 0.5145 (0.4722)  loss_classifier: 0.1231 (0.0988)  loss_box_reg: 0.0358 (0.0296)  loss_objectness: 0.0079 (0.0149)  loss_rpn_box_reg: 0.3101 (0.3288)  time: 0.2178  data: 0.0039  max mem: 5565\n",
      "Epoch: [5]  [2771/2772]  eta: 0:00:00  lr: 0.000010  loss: 0.4180 (0.4707)  loss_classifier: 0.0618 (0.0983)  loss_box_reg: 0.0142 (0.0294)  loss_objectness: 0.0248 (0.0150)  loss_rpn_box_reg: 0.2899 (0.3281)  time: 0.2153  data: 0.0040  max mem: 5565\n",
      "Epoch: [5] Total time: 0:10:07 (0.2193 s / it)\n",
      "epoch 5 loss 0.4760380983352661 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446), tensor(5.6091)]\n",
      "Epoch: [6]  [   0/2772]  eta: 0:24:37  lr: 0.000001  loss: 0.6254 (0.6254)  loss_classifier: 0.1297 (0.1297)  loss_box_reg: 0.0420 (0.0420)  loss_objectness: 0.0037 (0.0037)  loss_rpn_box_reg: 0.4500 (0.4500)  time: 0.5329  data: 0.3080  max mem: 5565\n",
      "Epoch: [6]  [ 500/2772]  eta: 0:08:15  lr: 0.000001  loss: 0.3823 (0.4447)  loss_classifier: 0.0625 (0.0917)  loss_box_reg: 0.0248 (0.0289)  loss_objectness: 0.0068 (0.0144)  loss_rpn_box_reg: 0.2567 (0.3097)  time: 0.2166  data: 0.0042  max mem: 5565\n",
      "Epoch: [6]  [1000/2772]  eta: 0:06:26  lr: 0.000001  loss: 0.4091 (0.4511)  loss_classifier: 0.0784 (0.0937)  loss_box_reg: 0.0321 (0.0293)  loss_objectness: 0.0033 (0.0152)  loss_rpn_box_reg: 0.2368 (0.3128)  time: 0.2173  data: 0.0043  max mem: 5565\n",
      "Epoch: [6]  [1500/2772]  eta: 0:04:36  lr: 0.000001  loss: 0.4658 (0.4516)  loss_classifier: 0.1146 (0.0937)  loss_box_reg: 0.0395 (0.0294)  loss_objectness: 0.0050 (0.0150)  loss_rpn_box_reg: 0.2966 (0.3134)  time: 0.2178  data: 0.0043  max mem: 5565\n",
      "Epoch: [6]  [2000/2772]  eta: 0:02:47  lr: 0.000001  loss: 0.4686 (0.4500)  loss_classifier: 0.1063 (0.0942)  loss_box_reg: 0.0196 (0.0296)  loss_objectness: 0.0146 (0.0147)  loss_rpn_box_reg: 0.2848 (0.3116)  time: 0.2162  data: 0.0042  max mem: 5565\n",
      "Epoch: [6]  [2500/2772]  eta: 0:00:59  lr: 0.000001  loss: 0.4608 (0.4488)  loss_classifier: 0.0813 (0.0939)  loss_box_reg: 0.0180 (0.0294)  loss_objectness: 0.0134 (0.0149)  loss_rpn_box_reg: 0.3394 (0.3105)  time: 0.2176  data: 0.0044  max mem: 5565\n",
      "Epoch: [6]  [2771/2772]  eta: 0:00:00  lr: 0.000001  loss: 0.4382 (0.4497)  loss_classifier: 0.1055 (0.0943)  loss_box_reg: 0.0320 (0.0296)  loss_objectness: 0.0054 (0.0148)  loss_rpn_box_reg: 0.2994 (0.3110)  time: 0.2175  data: 0.0044  max mem: 5565\n",
      "Epoch: [6] Total time: 0:10:01 (0.2171 s / it)\n",
      "epoch 6 loss 0.42026057839393616 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446), tensor(5.6091), tensor(4.9758)]\n",
      "Epoch: [7]  [   0/2772]  eta: 0:19:12  lr: 0.000001  loss: 0.5039 (0.5039)  loss_classifier: 0.0959 (0.0959)  loss_box_reg: 0.0244 (0.0244)  loss_objectness: 0.0277 (0.0277)  loss_rpn_box_reg: 0.3559 (0.3559)  time: 0.4159  data: 0.1956  max mem: 5565\n",
      "Epoch: [7]  [ 500/2772]  eta: 0:08:17  lr: 0.000001  loss: 0.4156 (0.4567)  loss_classifier: 0.0567 (0.0975)  loss_box_reg: 0.0211 (0.0310)  loss_objectness: 0.0148 (0.0136)  loss_rpn_box_reg: 0.2556 (0.3145)  time: 0.2165  data: 0.0042  max mem: 5565\n",
      "Epoch: [7]  [1000/2772]  eta: 0:06:29  lr: 0.000001  loss: 0.4076 (0.4472)  loss_classifier: 0.0725 (0.0934)  loss_box_reg: 0.0310 (0.0295)  loss_objectness: 0.0228 (0.0142)  loss_rpn_box_reg: 0.2624 (0.3101)  time: 0.2181  data: 0.0044  max mem: 5565\n",
      "Epoch: [7]  [1500/2772]  eta: 0:04:38  lr: 0.000001  loss: 0.3965 (0.4454)  loss_classifier: 0.0947 (0.0930)  loss_box_reg: 0.0311 (0.0293)  loss_objectness: 0.0071 (0.0144)  loss_rpn_box_reg: 0.2762 (0.3088)  time: 0.2171  data: 0.0039  max mem: 5565\n",
      "Epoch: [7]  [2000/2772]  eta: 0:02:49  lr: 0.000001  loss: 0.3963 (0.4476)  loss_classifier: 0.0967 (0.0942)  loss_box_reg: 0.0306 (0.0300)  loss_objectness: 0.0046 (0.0144)  loss_rpn_box_reg: 0.2515 (0.3090)  time: 0.2156  data: 0.0040  max mem: 5565\n",
      "Epoch: [7]  [2500/2772]  eta: 0:00:59  lr: 0.000001  loss: 0.3589 (0.4457)  loss_classifier: 0.0597 (0.0940)  loss_box_reg: 0.0189 (0.0301)  loss_objectness: 0.0183 (0.0145)  loss_rpn_box_reg: 0.2380 (0.3072)  time: 0.2141  data: 0.0038  max mem: 5565\n",
      "Epoch: [7]  [2771/2772]  eta: 0:00:00  lr: 0.000001  loss: 0.4532 (0.4460)  loss_classifier: 0.0809 (0.0941)  loss_box_reg: 0.0327 (0.0302)  loss_objectness: 0.0085 (0.0145)  loss_rpn_box_reg: 0.2706 (0.3072)  time: 0.2163  data: 0.0041  max mem: 5565\n",
      "Epoch: [7] Total time: 0:10:05 (0.2184 s / it)\n",
      "epoch 7 loss 0.6387823820114136 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446), tensor(5.6091), tensor(4.9758), tensor(4.8037)]\n",
      "Epoch: [8]  [   0/2772]  eta: 0:19:45  lr: 0.000001  loss: 0.7396 (0.7396)  loss_classifier: 0.2057 (0.2057)  loss_box_reg: 0.0419 (0.0419)  loss_objectness: 0.0314 (0.0314)  loss_rpn_box_reg: 0.4606 (0.4606)  time: 0.4276  data: 0.2068  max mem: 5565\n",
      "Epoch: [8]  [ 500/2772]  eta: 0:08:15  lr: 0.000001  loss: 0.4160 (0.4609)  loss_classifier: 0.0972 (0.0955)  loss_box_reg: 0.0204 (0.0300)  loss_objectness: 0.0128 (0.0146)  loss_rpn_box_reg: 0.2823 (0.3207)  time: 0.2169  data: 0.0044  max mem: 5565\n",
      "Epoch: [8]  [1000/2772]  eta: 0:06:25  lr: 0.000001  loss: 0.3806 (0.4515)  loss_classifier: 0.0763 (0.0961)  loss_box_reg: 0.0217 (0.0304)  loss_objectness: 0.0125 (0.0143)  loss_rpn_box_reg: 0.2719 (0.3107)  time: 0.2161  data: 0.0042  max mem: 5565\n",
      "Epoch: [8]  [1500/2772]  eta: 0:04:36  lr: 0.000001  loss: 0.4153 (0.4477)  loss_classifier: 0.0797 (0.0953)  loss_box_reg: 0.0217 (0.0304)  loss_objectness: 0.0045 (0.0146)  loss_rpn_box_reg: 0.2835 (0.3074)  time: 0.2174  data: 0.0043  max mem: 5565\n",
      "Epoch: [8]  [2000/2772]  eta: 0:02:47  lr: 0.000001  loss: 0.4168 (0.4450)  loss_classifier: 0.0853 (0.0943)  loss_box_reg: 0.0267 (0.0303)  loss_objectness: 0.0133 (0.0146)  loss_rpn_box_reg: 0.3054 (0.3058)  time: 0.2156  data: 0.0041  max mem: 5565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8]  [2500/2772]  eta: 0:00:59  lr: 0.000001  loss: 0.4245 (0.4440)  loss_classifier: 0.0925 (0.0939)  loss_box_reg: 0.0382 (0.0301)  loss_objectness: 0.0035 (0.0144)  loss_rpn_box_reg: 0.2667 (0.3055)  time: 0.2155  data: 0.0039  max mem: 5565\n",
      "Epoch: [8]  [2771/2772]  eta: 0:00:00  lr: 0.000001  loss: 0.4139 (0.4426)  loss_classifier: 0.0944 (0.0936)  loss_box_reg: 0.0295 (0.0301)  loss_objectness: 0.0042 (0.0145)  loss_rpn_box_reg: 0.2739 (0.3044)  time: 0.2151  data: 0.0040  max mem: 5565\n",
      "Epoch: [8] Total time: 0:10:02 (0.2172 s / it)\n",
      "epoch 8 loss 0.3274722099304199 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446), tensor(5.6091), tensor(4.9758), tensor(4.8037), tensor(5.4931)]\n",
      "Epoch: [9]  [   0/2772]  eta: 0:19:09  lr: 0.000000  loss: 0.6829 (0.6829)  loss_classifier: 0.2133 (0.2133)  loss_box_reg: 0.0749 (0.0749)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.3912 (0.3912)  time: 0.4146  data: 0.1927  max mem: 5565\n",
      "Epoch: [9]  [ 500/2772]  eta: 0:08:15  lr: 0.000000  loss: 0.3957 (0.4417)  loss_classifier: 0.0793 (0.0924)  loss_box_reg: 0.0300 (0.0293)  loss_objectness: 0.0086 (0.0157)  loss_rpn_box_reg: 0.2743 (0.3042)  time: 0.2192  data: 0.0044  max mem: 5565\n",
      "Epoch: [9]  [1000/2772]  eta: 0:06:26  lr: 0.000000  loss: 0.4492 (0.4450)  loss_classifier: 0.0823 (0.0940)  loss_box_reg: 0.0228 (0.0302)  loss_objectness: 0.0053 (0.0152)  loss_rpn_box_reg: 0.2682 (0.3055)  time: 0.2160  data: 0.0042  max mem: 5565\n",
      "Epoch: [9]  [1500/2772]  eta: 0:04:40  lr: 0.000000  loss: 0.4070 (0.4453)  loss_classifier: 0.0741 (0.0940)  loss_box_reg: 0.0266 (0.0305)  loss_objectness: 0.0105 (0.0151)  loss_rpn_box_reg: 0.3121 (0.3057)  time: 0.2173  data: 0.0044  max mem: 5565\n",
      "Epoch: [9]  [2000/2772]  eta: 0:02:49  lr: 0.000000  loss: 0.4357 (0.4418)  loss_classifier: 0.0777 (0.0930)  loss_box_reg: 0.0291 (0.0305)  loss_objectness: 0.0073 (0.0150)  loss_rpn_box_reg: 0.2563 (0.3033)  time: 0.2155  data: 0.0041  max mem: 5565\n",
      "Epoch: [9]  [2500/2772]  eta: 0:00:59  lr: 0.000000  loss: 0.4053 (0.4396)  loss_classifier: 0.0711 (0.0922)  loss_box_reg: 0.0164 (0.0300)  loss_objectness: 0.0035 (0.0150)  loss_rpn_box_reg: 0.2721 (0.3023)  time: 0.2172  data: 0.0044  max mem: 5565\n",
      "Epoch: [9]  [2771/2772]  eta: 0:00:00  lr: 0.000000  loss: 0.4073 (0.4402)  loss_classifier: 0.0946 (0.0926)  loss_box_reg: 0.0355 (0.0302)  loss_objectness: 0.0120 (0.0150)  loss_rpn_box_reg: 0.2546 (0.3025)  time: 0.2195  data: 0.0045  max mem: 5565\n",
      "Epoch: [9] Total time: 0:10:06 (0.2189 s / it)\n",
      "epoch 9 loss 0.44527891278266907 total_ats [tensor(2.4805), tensor(2.8058), tensor(3.2230), tensor(5.0569), tensor(3.9446), tensor(5.6091), tensor(4.9758), tensor(4.8037), tensor(5.4931), tensor(4.5205)]\n"
     ]
    }
   ],
   "source": [
    "losses, total_ats, best_model_wts = train_val(feature_extractor, model,\n",
    "                                              train_loader, val_loader,\n",
    "                                              num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPFR4eCVfF2e",
    "outputId": "e4181be0-88b1-479e-be4c-1f6481744565"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(losses, open('/scratch/nhl256/dl_project/model/object_detection_resnet18_0509_trainlosses.pickle', \"wb\"))\n",
    "pickle.dump(total_ats, open('/scratch/nhl256/dl_project/model/object_detection_resnet18_0509_evalTotalAts.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sesiU-p20F"
   },
   "source": [
    "## Evaluate - IoU by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N7S-1srTbRg"
   },
   "outputs": [],
   "source": [
    "def prepare_pred_results(predictions):\n",
    "    pred_boxes = []\n",
    "    pred_labels = []\n",
    "    pred_scores = []\n",
    "    for prediction in predictions:\n",
    "        #print(prediction)\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        pred_boxes.append(boxes)\n",
    "        pred_labels.append(labels)\n",
    "        pred_scores.append(scores)\n",
    "\n",
    "    return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "def reorder_coord(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((ymin, xmin, ymax, xmax), dim=1)\n",
    "\n",
    "def prepare_gt(targets):\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        labels = target[\"labels\"].tolist()\n",
    "        gt_boxes.append(boxes)\n",
    "        gt_labels.append(labels)\n",
    "    return gt_boxes, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2a_EldORlxqm"
   },
   "outputs": [],
   "source": [
    "# Make sure that bbox_a, bbox_b = np.array\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    #print(type(bbox_a), type(bbox_b))\n",
    "    bbox_a = np.array(bbox_a)\n",
    "    bbox_b = np.array(bbox_b)\n",
    "\n",
    "    # print(type(bbox_a), type(bbox_b))\n",
    "    # print(bbox_a.shape, bbox_b.shape)\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # top left (i.e., ymin, xmin)\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right (i.e., ymax, xmax)\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    # Area of intersection: (tl < br) = bool, (br-tl) = (ymax-ymin) \n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahoOrX-8ZFzT"
   },
   "outputs": [],
   "source": [
    "def cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres=0.5):\n",
    "    iou_table = bbox_iou(pred_bbox_c, gt_bbox_c)\n",
    "    num_pred_bboxes = iou_table.shape[0]\n",
    "    num_gt_bboxes = iou_table.shape[1]\n",
    "    TP = np.zeros(num_pred_bboxes)\n",
    "    FP = np.zeros(num_pred_bboxes)\n",
    "    # For each pred_bounding box:\n",
    "      # Find the most relevant gt_bbox (i.e., the gt_bbox with max IoU)\n",
    "      # If IoU < threshold, then flag it as FP\n",
    "      # If IoU >= threshold, then:\n",
    "        # If that gt_bbox already has already matched with another pred_bbox:\n",
    "          # Flag it as FP\n",
    "        # Else:\n",
    "          # Flag it as TP\n",
    "\n",
    "    # TP only happens if the pred_bbox mathes with a gt_bbox\n",
    "    for i in range(num_pred_bboxes):\n",
    "        gt_bbox_index = np.argmax(iou_table[i])\n",
    "        best_pred_bbox_index_for_selected_gt_bbox = np.argmax(iou_table[:,gt_bbox_index])\n",
    "        if iou_table[i, gt_bbox_index] > iou_thres \\\n",
    "            and gt_bbox_index == best_pred_bbox_index_for_selected_gt_bbox:\n",
    "            TP[i] = 1\n",
    "        else:\n",
    "            FP[i] = 1\n",
    "\n",
    "    TP_cum = np.sum(TP)\n",
    "    FP_cum = np.sum(FP)\n",
    "\n",
    "    if (TP_cum + FP_cum) != num_pred_bboxes:\n",
    "        print(\"WRONG CALCULATION OF FP\")\n",
    "    return TP_cum, FP_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzx15kDKS3Zt"
   },
   "outputs": [],
   "source": [
    "# Test for cal_TP_FP_iou\n",
    "\n",
    "def inspect_call_TP_FP_iou(test_images, test_targets):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            # print(num_gt_bboxes)\n",
    "            # print(num_pred_bboxes)\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                print(class_TP + class_FP == num_pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8N2DCbcJe92u"
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print('Iter {}'.format(i))\n",
    "#     test_images, test_targets = next(iter(test_loader))\n",
    "#     inspect_call_TP_FP_iou(test_images, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "honU3_7QQLwr"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(predictions, test_targets, res, iou_thres=0.5):\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "    # res stores the TP_FP dict for each class\n",
    "    # Each TP_FP dict stores the TP_FP for each class \n",
    "    \n",
    "    batch_total_TP = 0\n",
    "    batch_total_FP = 0\n",
    "    batch_total_FN = 0\n",
    "    batch_total_num_object = 0\n",
    "    batch_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            #print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                #print(class_TP + class_FP == num_pred_bboxes)\n",
    "\n",
    "            batch_total_TP += class_TP\n",
    "            batch_total_FP += class_FP\n",
    "            batch_total_FN += class_FN\n",
    "            batch_total_num_object += num_gt_bboxes\n",
    "\n",
    "            batch_res[c]['TP'] += class_TP\n",
    "            batch_res[c]['FP'] += class_FP\n",
    "            batch_res[c]['FN'] += class_FN\n",
    "\n",
    "            res[c]['TP'] += class_TP\n",
    "            res[c]['FP'] += class_FP\n",
    "            res[c]['FN'] += class_FN\n",
    "            \n",
    "    return res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nji9KKjySsLT"
   },
   "outputs": [],
   "source": [
    "# Inspect evaluate_one_batch\n",
    "def inspect_evaluate_one_batch(test_images, test_targets, final_res):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "    return final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "qfzs9R0nS5Wr",
    "outputId": "bf329a68-8d6e-4aac-9662-391a10a96fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "batch 1\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 19}, 3: {'TP': 0, 'FP': 0, 'FN': 0}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 2}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 35}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "final_TP = 0\n",
    "final_FP = 0\n",
    "final_FN = 0\n",
    "final_num_objects = 0\n",
    "\n",
    "# test for 2 batches\n",
    "for i in range(2):\n",
    "    test_images, test_targets = next(iter(test_loader))\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = inspect_evaluate_one_batch(test_images, test_targets, final_res)\n",
    "\n",
    "    print('batch {}'.format(i))\n",
    "\n",
    "    \n",
    "    print(batch_total_TP, batch_total_FN, batch_total_num_object)\n",
    "    print('cur batch res:', batch_res)\n",
    "    print('final res after this batch:', final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hf3Cii6S5gf"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, iou_thres=0.5):\n",
    "    # Evaluate. for all data point in the evaluaton set\n",
    "    final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "    final_TP = 0\n",
    "    final_FP = 0\n",
    "    final_FN = 0\n",
    "    final_num_objects = 0\n",
    "\n",
    "    for iter_, (test_images, test_targets) in enumerate(test_loader):\n",
    "        # if iter_ % 50 == 0:\n",
    "        #     print('iter', iter_)\n",
    "        #print('iter', iter_)\n",
    "        test_images = torch.stack(test_images)\n",
    "        #print(test_images.shape)\n",
    "        test_images = prepare_inputs(test_images)\n",
    "        #print(test_images[0].shape)\n",
    "\n",
    "        test_images = list(image.to(device) for image in test_images)\n",
    "        test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "        model.eval()\n",
    "        predictions = model(test_images)\n",
    "\n",
    "        # Evaluate for one batch\n",
    "        final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "                    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "        \n",
    "        final_TP += batch_total_TP\n",
    "        final_FP += batch_total_FP\n",
    "        final_FN += batch_total_FN\n",
    "        final_num_objects += batch_total_num_object\n",
    "\n",
    "    return final_res, final_TP, final_FP, final_FN, final_num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Zop8Z75IS5ep",
    "outputId": "ed65157d-23c8-4f3f-8a91-00a27a6ee63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 50\n",
      "iter 100\n"
     ]
    }
   ],
   "source": [
    "final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFT09TgzS5cW"
   },
   "outputs": [],
   "source": [
    "def evaluate_threst_score(TP, FP, FN):\n",
    "    return (TP / (TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ejRrnjTloLBc",
    "outputId": "4c05fac2-7eb0-450a-b874-855a51e1b4e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       " 1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 2: {'FN': 3074, 'FP': 0, 'TP': 0},\n",
       " 3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       " 4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       " 5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       " 7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 8: {'FN': 0, 'FP': 0, 'TP': 0}}"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve5Zlqi-dvxf"
   },
   "source": [
    "## Train and Evaluate for Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_4-JrdeH59"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4fAgXgidudg"
   },
   "outputs": [],
   "source": [
    "def train_eval(model, train_loader, test_loader, iou_thres=0.5, num_epochs=10):\n",
    "    train_losses = []\n",
    "    eval_threatscores = []\n",
    "    eval_final_res = []\n",
    "    best_eval_ts = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(final_TP, final_FP, final_FN, final_num_objects)\n",
    "        eval_final_res.append(final_res)\n",
    "        eval_ts = evaluate_threst_score(final_TP, final_FP, final_FN)\n",
    "        eval_threatscores.append(eval_ts)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"epoch: {} eval_ts {}\".format(epoch, eval_ts))\n",
    "\n",
    "        if eval_ts > best_eval_ts:\n",
    "            best_eval_ts = eval_ts \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return model, best_model_wts, train_losses, eval_final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsAuhVThduhA"
   },
   "outputs": [],
   "source": [
    "model, best_model_wts, train_losses, eval_final_res = train_eval(model, train_loader,\n",
    "                                                                 test_loader, iou_thres=0.5,\n",
    "                                                                 num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "Naw6li0b09RY",
    "outputId": "70a1d53f-67ed-4e70-b5ad-3f8377c580ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3073.0, 'FP': 12599.0, 'TP': 1.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}},\n",
       " {0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3070.0, 'FP': 846.0, 'TP': 4.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}}]"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llVJ0SeDM40i"
   },
   "source": [
    "## Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xzLLZktM4aI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7w8Da6JM8or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXslpd4-d-rQ"
   },
   "source": [
    "## Customize Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGCMc2Ppd-rR"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IC8olie21GE"
   },
   "source": [
    "#### 1. Mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "cukXGxWk1WtO",
    "outputId": "1c2db8fe-1068-4235-9348-d70fcb3ea02b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47b1e48fc071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n\u001b[1;32m      4\u001b[0m                                     aspect_ratios=((0.5, 1.0, 2.0),))\n\u001b[1;32m      5\u001b[0m roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                    num_classes=21,\n",
    "                    rpn_anchor_generator=anchor_generator,\n",
    "                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVBYaALa2xOK"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7XYTwLa26Mk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmOGwmnO26uZ"
   },
   "source": [
    "#### 2. CustomVGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIB1q2yd-rl"
   },
   "outputs": [],
   "source": [
    "def customize_VGG16():\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    \n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "    \n",
    "    classifier = list(classifier)\n",
    "    # delete the Linear layer\n",
    "    del classifier[6]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    #freeze top4 conv layer\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "    features = nn.Sequential(*features)\n",
    "        \n",
    "    return features, classifier\n",
    "backbone, box_head = customize_VGG16()\n",
    "backbone.out_channels = 512\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pb3n-b1_pNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "objectDectionFastRCNN_050220.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
