{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1232,
     "status": "error",
     "timestamp": 1587749437462,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "X9PMrKKreF3N",
    "outputId": "7daa6757-1d1a-44ef-f8db-3844200592dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "os.chdir('/scratch/nhl256/dl_project/code/')\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import *\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_utils.py  data_helper.py  objectDectionFastRCNN.ipynb  transforms.py\r\n",
      "data\t       helper.py       __pycache__\t\t    utils.py\r\n"
     ]
    }
   ],
   "source": [
    "# !mkdir data\n",
    "# !ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip '/scratch/jz3224/DLSP20Dataset/student_data.zip' -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/jz3224/DLSP20Dataset/student_data.zip\r\n"
     ]
    }
   ],
   "source": [
    "#!ls '/scratch/jz3224/DLSP20Dataset/student_data.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23wjtm3ufX2t"
   },
   "source": [
    "# Test an example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-mow3YMfT-_"
   },
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "# labels = torch.randint(1, 91, (4, 11))\n",
    "# images = list(image for image in images)\n",
    "# targets = []\n",
    "# for i in range(len(images)):\n",
    "#     d = {}\n",
    "#     d['boxes'] = boxes[i]\n",
    "#     d['labels'] = labels[i]\n",
    "#     targets.append(d)\n",
    "# output = model(images, targets)\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1587748816905,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "A16_Anied-mJ",
    "outputId": "93200542-1dee-4080-84e3-94cdeada709d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "\n",
    "# image_folder = '../data'\n",
    "# annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "image_folder = 'data/data'\n",
    "annotation_csv = 'data/data/annotation.csv'\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# image_folder = '/Users/nhungle/Downloads/dl20_data'\n",
    "# annotation_csv = '/Users/nhungle/Downloads/dl20_data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1587748833605,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "ri_jT4rHM6rb",
    "outputId": "3db2587a-9933-44ce-846f-1ca4559442ae"
   },
   "outputs": [],
   "source": [
    "!#ls 'data/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-PzB4lPd-n7"
   },
   "source": [
    "# Labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mk75N39Cd-n8"
   },
   "outputs": [],
   "source": [
    "def inspect_target(index, labeled_scene_index):\n",
    "    NUM_SAMPLE_PER_SCENE = 126\n",
    "    NUM_IMAGE_PER_SAMPLE = 6\n",
    "    image_names = [\n",
    "        'CAM_FRONT_LEFT.jpeg',\n",
    "        'CAM_FRONT.jpeg',\n",
    "        'CAM_FRONT_RIGHT.jpeg',\n",
    "        'CAM_BACK_LEFT.jpeg',\n",
    "        'CAM_BACK.jpeg',\n",
    "        'CAM_BACK_RIGHT.jpeg',\n",
    "        ]\n",
    "    scene_index = labeled_scene_index \n",
    "    scene_id = scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "    sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "    sample_path = os.path.join(image_folder, f'scene_{scene_id}', f'sample_{sample_id}') \n",
    "    images = []\n",
    "    for image_name in image_names:\n",
    "        image_path = os.path.join(sample_path, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        images.append(transform(image))\n",
    "    image_tensor = torch.stack(images)\n",
    "    annotation_file = annotation_csv \n",
    "    annotation_dataframe = pd.read_csv(annotation_file)\n",
    "    data_entries = annotation_dataframe[(annotation_dataframe['scene'] == scene_id) & (annotation_dataframe['sample'] == sample_id)]\n",
    "    corners = data_entries[['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']].to_numpy()\n",
    "    categories = data_entries.category_id.to_numpy()\n",
    "    num_objects = len(categories)\n",
    "    boxes = []\n",
    "    for i in range(num_objects):\n",
    "        xmin = min(corners[i][:4])\n",
    "        xmax = max(corners[i][:4])\n",
    "        ymin = min(corners[i][4:])\n",
    "        ymax = max(corners[i][4:])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return data_entries, image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom2g6MJd-oP"
   },
   "outputs": [],
   "source": [
    "train_labeled_scene_index = np.arange(106, 125)\n",
    "val_labeled_scene_index = np.arange(125, 131)\n",
    "test_labeled_scene_index = np.arange(131, 134)\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_valset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "val_loader = torch.utils.data.DataLoader(fasterRCNN_valset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HFKc2hcbN3e"
   },
   "outputs": [],
   "source": [
    "train_loader.__len__()\n",
    "sample, targets = iter(train_loader).next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1587748954425,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "Bpy2gf9mNk8h",
    "outputId": "9c588be8-ab52-48b4-defd-fa23fe2b82d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = targets[0]['image_id'].item()\n",
    "data_entries, idx_tensor = inspect_target(index, train_labeled_scene_index)\n",
    "data_entries[\"category_id\"].values == targets[0]['labels'].data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq87_1TsZdxH"
   },
   "source": [
    "## Prepare inputs for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3v00HG3ZdJP"
   },
   "outputs": [],
   "source": [
    "def extract_features(one_sample):\n",
    "    feature_extractor = torchvision.models.resnet18(pretrained=False)\n",
    "    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2])\n",
    "    #feature_extractor.to(device)\n",
    "    # for param in feature_extractor.parameters():\n",
    "    #     param.requires_grad = True\n",
    "    return feature_extractor(one_sample)\n",
    "\n",
    "\n",
    "def concat_features(features, dim = 2):\n",
    "    #dim 0 ==> stacking the images in the channel dimension\n",
    "    #dim 1 ==> stacking the images in row dimension\n",
    "    #dim 2 ==> stacking the images in column dimension\n",
    "    tensor_tuples = torch.unbind(features, dim=0)\n",
    "    concatenated_fm = torch.cat(tensor_tuples, dim=dim)\n",
    "    return concatenated_fm \n",
    "\n",
    "def prepare_inputs(sample):\n",
    "    \"\"\"\n",
    "    Input: samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n",
    "    Output: a list of batch_size tensor, each tensor with size [512, 16, 114]\n",
    "    \"\"\"\n",
    "    batchsize = sample.shape[0]\n",
    "    fe_batch = []\n",
    "    for i in range(batchsize):\n",
    "        image_tensor = sample[i]\n",
    "        features = extract_features(image_tensor)\n",
    "        #print(features.shape)\n",
    "        features = concat_features(features)\n",
    "        features = features.view(3, 512, 160)\n",
    "        #print(features.shape)\n",
    "        fe_batch.append(features)\n",
    "    \n",
    "    return fe_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuAHlunjZlLB"
   },
   "outputs": [],
   "source": [
    "# sample = torch.stack(sample)\n",
    "# images = prepare_inputs(sample)\n",
    "# images = list(image.to(device) for image in images)\n",
    "# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eY5lf6lfbZTU"
   },
   "source": [
    "## Train and Evaluate for 1 sence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2p5G8s6cRBv"
   },
   "outputs": [],
   "source": [
    "# Refer to: https://github.com/pytorch/vision/blob/master/references/detection/engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RCAioWzd_RC"
   },
   "outputs": [],
   "source": [
    "# train_labeled_scene_index = np.arange(131, 132)\n",
    "# test_labeled_scene_index = np.arange(132, 133)\n",
    "# fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "#                                   annotation_file=annotation_csv,\n",
    "#                                   scene_index=train_labeled_scene_index,\n",
    "#                                   transform=transform,\n",
    "#                                   extra_info=True\n",
    "#                                  )\n",
    "# train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "#                                           batch_size=1,\n",
    "#                                           shuffle=True,\n",
    "#                                           num_workers=2, collate_fn=collate_fn)\n",
    "# fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "#                                   annotation_file=annotation_csv,\n",
    "#                                   scene_index=test_labeled_scene_index,\n",
    "#                                   transform=transform,\n",
    "#                                   extra_info=True\n",
    "#                                  )\n",
    "# test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "#                                           batch_size=1,\n",
    "#                                           shuffle=True,\n",
    "#                                           num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 432,
     "status": "error",
     "timestamp": 1587749031431,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "zEsCSbEEdsgC",
    "outputId": "cb093db4-2bf6-4b3b-8de8-31e9fcce626a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0mfqyfzbYgy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/nhl256/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:03<00:00, 53.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model = model.to(device)\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# # and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                 step_size=3,\n",
    "#                                                 gamma=0.1)\n",
    "\n",
    "# # let's train it for 10 epochs\n",
    "# num_epochs = 1\n",
    "# epoch = 0\n",
    "# print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7REJR4wZiZEv"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = torch.stack(images)\n",
    "        #print(images.shape)\n",
    "        images = prepare_inputs(images)\n",
    "        #print(images[0].shape)\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        #print(losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4077,
     "status": "error",
     "timestamp": 1587745472535,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "vNgNdjDyimY4",
    "outputId": "26afdfde-025b-451f-a17e-6a3bbadbd308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/126]  eta: 0:03:42  lr: 0.000045  loss: 0.6737 (0.6737)  loss_classifier: 0.1618 (0.1618)  loss_box_reg: 0.0234 (0.0234)  loss_objectness: 0.0381 (0.0381)  loss_rpn_box_reg: 0.4504 (0.4504)  time: 1.7663  data: 0.2147  max mem: 1302\n",
      "Epoch: [0]  [ 20/126]  eta: 0:02:24  lr: 0.000844  loss: 0.6608 (0.6817)  loss_classifier: 0.2318 (0.2304)  loss_box_reg: 0.0281 (0.0323)  loss_objectness: 0.0456 (0.0450)  loss_rpn_box_reg: 0.3588 (0.3739)  time: 1.3432  data: 0.0035  max mem: 1302\n",
      "Epoch: [0]  [ 40/126]  eta: 0:01:55  lr: 0.001643  loss: 0.7009 (0.6976)  loss_classifier: 0.2472 (0.2370)  loss_box_reg: 0.0404 (0.0385)  loss_objectness: 0.0333 (0.0437)  loss_rpn_box_reg: 0.3607 (0.3784)  time: 1.3328  data: 0.0037  max mem: 1302\n",
      "Epoch: [0]  [ 60/126]  eta: 0:01:28  lr: 0.002443  loss: 0.6858 (0.6956)  loss_classifier: 0.2119 (0.2279)  loss_box_reg: 0.0345 (0.0371)  loss_objectness: 0.0378 (0.0430)  loss_rpn_box_reg: 0.3973 (0.3876)  time: 1.3332  data: 0.0037  max mem: 1302\n",
      "Epoch: [0]  [ 80/126]  eta: 0:01:01  lr: 0.003242  loss: 0.6863 (0.6969)  loss_classifier: 0.2477 (0.2295)  loss_box_reg: 0.0385 (0.0384)  loss_objectness: 0.0391 (0.0421)  loss_rpn_box_reg: 0.3570 (0.3869)  time: 1.3347  data: 0.0037  max mem: 1302\n",
      "Epoch: [0]  [100/126]  eta: 0:00:34  lr: 0.004041  loss: 0.6833 (0.6953)  loss_classifier: 0.2358 (0.2292)  loss_box_reg: 0.0417 (0.0391)  loss_objectness: 0.0284 (0.0412)  loss_rpn_box_reg: 0.3616 (0.3858)  time: 1.3329  data: 0.0037  max mem: 1302\n",
      "Epoch: [0]  [120/126]  eta: 0:00:08  lr: 0.004840  loss: 0.6915 (0.6939)  loss_classifier: 0.2136 (0.2279)  loss_box_reg: 0.0385 (0.0400)  loss_objectness: 0.0421 (0.0410)  loss_rpn_box_reg: 0.3770 (0.3849)  time: 1.3315  data: 0.0037  max mem: 1302\n",
      "Epoch: [0]  [125/126]  eta: 0:00:01  lr: 0.005000  loss: 0.7010 (0.6942)  loss_classifier: 0.2136 (0.2271)  loss_box_reg: 0.0385 (0.0400)  loss_objectness: 0.0341 (0.0409)  loss_rpn_box_reg: 0.3800 (0.3861)  time: 1.3324  data: 0.0037  max mem: 1302\n",
      "Epoch: [0] Total time: 0:02:48 (1.3390 s / it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7010, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKLzAqS5jayO"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N7S-1srTbRg"
   },
   "outputs": [],
   "source": [
    "def prepare_pred_results(predictions):\n",
    "    pred_boxes = []\n",
    "    pred_labels = []\n",
    "    pred_scores = []\n",
    "    for prediction in predictions:\n",
    "        #print(prediction)\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        pred_boxes.append(boxes)\n",
    "        pred_labels.append(labels)\n",
    "        pred_scores.append(scores)\n",
    "\n",
    "    return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "def reorder_coord(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((ymin, xmin, ymax, xmax), dim=1)\n",
    "\n",
    "def prepare_gt(targets):\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        labels = target[\"labels\"].tolist()\n",
    "        gt_boxes.append(boxes)\n",
    "        gt_labels.append(labels)\n",
    "    return gt_boxes, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2a_EldORlxqm"
   },
   "outputs": [],
   "source": [
    "# Make sure that bbox_a, bbox_b = np.array\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    #print(type(bbox_a), type(bbox_b))\n",
    "    bbox_a = np.array(bbox_a)\n",
    "    bbox_b = np.array(bbox_b)\n",
    "\n",
    "    # print(type(bbox_a), type(bbox_b))\n",
    "    # print(bbox_a.shape, bbox_b.shape)\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # top left (i.e., ymin, xmin)\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right (i.e., ymax, xmax)\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    # Area of intersection: (tl < br) = bool, (br-tl) = (ymax-ymin) \n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahoOrX-8ZFzT"
   },
   "outputs": [],
   "source": [
    "def cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres=0.5):\n",
    "    iou_table = bbox_iou(pred_bbox_c, gt_bbox_c)\n",
    "    num_pred_bboxes = iou_table.shape[0]\n",
    "    num_gt_bboxes = iou_table.shape[1]\n",
    "    TP = np.zeros(num_pred_bboxes)\n",
    "    FP = np.zeros(num_pred_bboxes)\n",
    "    # For each pred_bounding box:\n",
    "      # Find the most relevant gt_bbox (i.e., the gt_bbox with max IoU)\n",
    "      # If IoU < threshold, then flag it as FP\n",
    "      # If IoU >= threshold, then:\n",
    "        # If that gt_bbox already has already matched with another pred_bbox:\n",
    "          # Flag it as FP\n",
    "        # Else:\n",
    "          # Flag it as TP\n",
    "\n",
    "    # TP only happens if the pred_bbox mathes with a gt_bbox\n",
    "    for i in range(num_pred_bboxes):\n",
    "        gt_bbox_index = np.argmax(iou_table[i])\n",
    "        best_pred_bbox_index_for_selected_gt_bbox = np.argmax(iou_table[:,gt_bbox_index])\n",
    "        if iou_table[i, gt_bbox_index] > iou_thres \\\n",
    "            and gt_bbox_index == best_pred_bbox_index_for_selected_gt_bbox:\n",
    "            TP[i] = 1\n",
    "        else:\n",
    "            FP[i] = 1\n",
    "\n",
    "    TP_cum = np.sum(TP)\n",
    "    FP_cum = np.sum(FP)\n",
    "\n",
    "    if (TP_cum + FP_cum) != num_pred_bboxes:\n",
    "        print(\"WRONG CALCULATION OF FP\")\n",
    "    return TP_cum, FP_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzx15kDKS3Zt"
   },
   "outputs": [],
   "source": [
    "# Test for cal_TP_FP_iou\n",
    "\n",
    "def inspect_call_TP_FP_iou(test_images, test_targets):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            # print(num_gt_bboxes)\n",
    "            # print(num_pred_bboxes)\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                print(class_TP + class_FP == num_pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8N2DCbcJe92u"
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print('Iter {}'.format(i))\n",
    "#     test_images, test_targets = next(iter(test_loader))\n",
    "#     inspect_call_TP_FP_iou(test_images, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "honU3_7QQLwr"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(predictions, test_targets, res, iou_thres=0.5):\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "    # res stores the TP_FP dict for each class\n",
    "    # Each TP_FP dict stores the TP_FP for each class \n",
    "    \n",
    "    batch_total_TP = 0\n",
    "    batch_total_FP = 0\n",
    "    batch_total_FN = 0\n",
    "    batch_total_num_object = 0\n",
    "    batch_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            #print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                #print(class_TP + class_FP == num_pred_bboxes)\n",
    "\n",
    "            batch_total_TP += class_TP\n",
    "            batch_total_FP += class_FP\n",
    "            batch_total_FN += class_FN\n",
    "            batch_total_num_object += num_gt_bboxes\n",
    "\n",
    "            batch_res[c]['TP'] += class_TP\n",
    "            batch_res[c]['FP'] += class_FP\n",
    "            batch_res[c]['FN'] += class_FN\n",
    "\n",
    "            res[c]['TP'] += class_TP\n",
    "            res[c]['FP'] += class_FP\n",
    "            res[c]['FN'] += class_FN\n",
    "            \n",
    "    return res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nji9KKjySsLT"
   },
   "outputs": [],
   "source": [
    "# Inspect evaluate_one_batch\n",
    "def inspect_evaluate_one_batch(test_images, test_targets, final_res):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "    return final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9747,
     "status": "ok",
     "timestamp": 1587738755570,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "qfzs9R0nS5Wr",
    "outputId": "bf329a68-8d6e-4aac-9662-391a10a96fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "0.0 31.0 31\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0.0, 'FP': 100.0, 'FN': 23.0}, 3: {'TP': 0, 'FP': 0, 'FN': 7}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0.0, 'FP': 100.0, 'FN': 23.0}, 3: {'TP': 0, 'FP': 0, 'FN': 7}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "batch 1\n",
      "0.0 29.0 29\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0.0, 'FP': 100.0, 'FN': 25.0}, 3: {'TP': 0, 'FP': 0, 'FN': 3}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 2}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0.0, 'FP': 200.0, 'FN': 48.0}, 3: {'TP': 0, 'FP': 0, 'FN': 10}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "final_TP = 0\n",
    "final_FP = 0\n",
    "final_FN = 0\n",
    "final_num_objects = 0\n",
    "\n",
    "# test for 2 batches\n",
    "for i in range(2):\n",
    "    test_images, test_targets = next(iter(test_loader))\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = inspect_evaluate_one_batch(test_images, test_targets, final_res)\n",
    "\n",
    "    print('batch {}'.format(i))\n",
    "\n",
    "    \n",
    "    print(batch_total_TP, batch_total_FN, batch_total_num_object)\n",
    "    print('cur batch res:', batch_res)\n",
    "    print('final res after this batch:', final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hf3Cii6S5gf"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, iou_thres=0.5):\n",
    "    # Evaluate. for all data point in the evaluaton set\n",
    "    final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "    final_TP = 0\n",
    "    final_FP = 0\n",
    "    final_FN = 0\n",
    "    final_num_objects = 0\n",
    "\n",
    "    for iter_, (test_images, test_targets) in enumerate(test_loader):\n",
    "        # if iter_ % 50 == 0:\n",
    "        #     print('iter', iter_)\n",
    "        #print('iter', iter_)\n",
    "        test_images = torch.stack(test_images)\n",
    "        #print(test_images.shape)\n",
    "        test_images = prepare_inputs(test_images)\n",
    "        #print(test_images[0].shape)\n",
    "\n",
    "        test_images = list(image.to(device) for image in test_images)\n",
    "        test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "        model.eval()\n",
    "        predictions = model(test_images)\n",
    "\n",
    "        # Evaluate for one batch\n",
    "        final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "                    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "        \n",
    "        final_TP += batch_total_TP\n",
    "        final_FP += batch_total_FP\n",
    "        final_FN += batch_total_FN\n",
    "        final_num_objects += batch_total_num_object\n",
    "\n",
    "    return final_res, final_TP, final_FP, final_FN, final_num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 95168,
     "status": "ok",
     "timestamp": 1587739135153,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "Zop8Z75IS5ep",
    "outputId": "ed65157d-23c8-4f3f-8a91-00a27a6ee63d"
   },
   "outputs": [],
   "source": [
    "final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFT09TgzS5cW"
   },
   "outputs": [],
   "source": [
    "def evaluate_threst_score(TP, FP, FN):\n",
    "    return (TP / (TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1587739141694,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "iEx2ls44dlXz",
    "outputId": "b2b39517-2567-4a24-f515-9114823598e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_threst_score(final_TP, final_FP, final_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1587739147790,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "ejRrnjTloLBc",
    "outputId": "4c05fac2-7eb0-450a-b874-855a51e1b4e9"
   },
   "outputs": [],
   "source": [
    "#final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve5Zlqi-dvxf"
   },
   "source": [
    "## Train and Evaluate for Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_scene_index = np.arange(106, 125)\n",
    "val_labeled_scene_index = np.arange(125, 131)\n",
    "test_labeled_scene_index = np.arange(131, 134)\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_valset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "val_loader = torch.utils.data.DataLoader(fasterRCNN_valset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_4-JrdeH59"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/nhl256/dl_project/model/object_detection_resnet18_epoch0.pth\n",
      "/scratch/nhl256/dl_project/model/object_detection_resnet18_epoch1.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print('/scratch/nhl256/dl_project/model/object_detection_resnet18_epoch{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4fAgXgidudg"
   },
   "outputs": [],
   "source": [
    "def train_eval(model, train_loader, val_loader, iou_thres=0.5, num_epochs=10):\n",
    "    train_losses = []\n",
    "    eval_threatscores = []\n",
    "    eval_final_res = []\n",
    "    best_eval_ts = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        \n",
    "        final_res, final_TP, final_FP, final_FN, final_num_objects \\\n",
    "        = evaluate_one_epoch(val_loader, iou_thres=0.5)\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(\"final_TP {}, final FP {}, final FN {}\".format(final_TP, final_FP, final_FN))\n",
    "        #print(final_TP, final_FP, final_FN, final_num_objects)\n",
    "        \n",
    "        eval_final_res.append(final_res)\n",
    "        eval_ts = evaluate_threst_score(final_TP, final_FP, final_FN)\n",
    "        eval_threatscores.append(eval_ts)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"epoch: {} eval_ts {}\".format(epoch, eval_ts))\n",
    "\n",
    "        if eval_ts > best_eval_ts:\n",
    "            best_eval_ts = eval_ts \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet18_epoch{}.pth'.format(epoch))\n",
    "\n",
    "    return model, best_model_wts, train_losses, eval_final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 837,
     "status": "error",
     "timestamp": 1587740890014,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "OsAuhVThduhA",
    "outputId": "48469848-d9f1-49f9-a91a-4fb9780fad72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2394]  eta: 1:22:31  lr: 0.000000  loss: 1.6030 (1.6030)  loss_classifier: 0.0847 (0.0847)  loss_box_reg: 0.0013 (0.0013)  loss_objectness: 0.8603 (0.8603)  loss_rpn_box_reg: 0.6566 (0.6566)  time: 2.0682  data: 0.3301  max mem: 2009\n",
      "Epoch: [0]  [ 500/2394]  eta: 0:43:39  lr: 0.000050  loss: 0.6638 (1.0159)  loss_classifier: 0.1096 (0.1735)  loss_box_reg: 0.0065 (0.0196)  loss_objectness: 0.0306 (0.2391)  loss_rpn_box_reg: 0.4105 (0.5837)  time: 1.4023  data: 0.0037  max mem: 2688\n"
     ]
    }
   ],
   "source": [
    "model, best_model_wts, train_losses, eval_final_res = train_eval(model, train_loader,\n",
    "                                                                 val_loader, iou_thres=0.5,\n",
    "                                                                 num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1587740004966,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "Naw6li0b09RY",
    "outputId": "70a1d53f-67ed-4e70-b5ad-3f8377c580ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1587679121033,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "DlPUDruY09M6",
    "outputId": "fb88fa50-a1d1-4336-9802-ff5c60c3f325",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXslpd4-d-rQ"
   },
   "source": [
    "## Customize Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGCMc2Ppd-rR"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IC8olie21GE"
   },
   "source": [
    "#### 1. Mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 934,
     "status": "error",
     "timestamp": 1587575181204,
     "user": {
      "displayName": "Nhung Hong Le",
      "photoUrl": "",
      "userId": "11957069750784655483"
     },
     "user_tz": 240
    },
    "id": "cukXGxWk1WtO",
    "outputId": "1c2db8fe-1068-4235-9348-d70fcb3ea02b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47b1e48fc071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n\u001b[1;32m      4\u001b[0m                                     aspect_ratios=((0.5, 1.0, 2.0),))\n\u001b[1;32m      5\u001b[0m roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                    num_classes=21,\n",
    "                    rpn_anchor_generator=anchor_generator,\n",
    "                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVBYaALa2xOK"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7XYTwLa26Mk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmOGwmnO26uZ"
   },
   "source": [
    "#### 2. CustomVGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIB1q2yd-rl"
   },
   "outputs": [],
   "source": [
    "def customize_VGG16():\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    \n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "    \n",
    "    classifier = list(classifier)\n",
    "    # delete the Linear layer\n",
    "    del classifier[6]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    #freeze top4 conv layer\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "    features = nn.Sequential(*features)\n",
    "        \n",
    "    return features, classifier\n",
    "backbone, box_head = customize_VGG16()\n",
    "backbone.out_channels = 512\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "objectDectionFastRCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
