{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "LMpEF8ZDd-lt",
    "outputId": "d4563cbf-c762-48a9-dbb7-6cb0ecc0c4fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "os.chdir('/scratch/nhl256/dl_project/code/')\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import *\n",
    "from nhung_data_helper import FastRCNNLabeledDataset\n",
    "from helper import *\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGxs-4igvAzk"
   },
   "source": [
    "### 05/02/20\n",
    "- Preprocess 6 images before passing through the model\n",
    "- Saving the feature extractor and the fasterRCNN model\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "X9PMrKKreF3N",
    "outputId": "38430281-d6fc-4ab0-d551-3da75203b517"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "\n",
    "# image_folder = '../data'\n",
    "# annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "image_folder = 'data/data'\n",
    "annotation_csv = 'data/data/annotation.csv'\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# image_folder = '/Users/nhungle/Downloads/dl20_data'\n",
    "# annotation_csv = '/Users/nhungle/Downloads/dl20_data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vmrTQbev_exi",
    "outputId": "eb026039-9b09-452f-e116-efaa4274ee56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htb4imOktWfj"
   },
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "random.seed(1008)\n",
    "random.shuffle(labeled_scene_index)\n",
    "train_labeled_scene_index = labeled_scene_index[:22]\n",
    "val_labeled_scene_index = labeled_scene_index[22:26]\n",
    "test_labeled_scene_index = labeled_scene_index[26:]\n",
    "BATCH_SIZE=1\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_valset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "val_loader = torch.utils.data.DataLoader(fasterRCNN_valset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gDRi-fS1TAlH",
    "outputId": "724493de-d954-425f-c0e9-09abdcaf4960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLuS_ttrTT_j"
   },
   "source": [
    "### Get Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88T92HOWTEx4"
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_extractor = torchvision.models.resnet18(pretrained=False)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2])\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVI7nFEITWEw"
   },
   "outputs": [],
   "source": [
    "def concat_features(features, dim = 2):\n",
    "    #dim 0 ==> stacking the images in the channel dimension\n",
    "    #dim 1 ==> stacking the images in row dimension\n",
    "    #dim 2 ==> stacking the images in column dimension\n",
    "    tensor_tuples = torch.unbind(features, dim=0)\n",
    "    concatenated_fm = torch.cat(tensor_tuples, dim=dim)\n",
    "    return concatenated_fm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTHbwweW-4B"
   },
   "source": [
    "### Inspect if it works on a pretrained FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HwKa9R3TXInF",
    "outputId": "35be123d-30e1-4db2-cedc-53da22551da2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 3, 256, 306])"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features)\n",
    "    features = features.view(3, 512, 160)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "CnnOF2RsT1j_",
    "outputId": "0eee6720-3222-45fe-def7-f737ad7976c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss_box_reg': tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_classifier': tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_objectness': tensor(1.0108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(0.8967, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "output1 = model(images, targets)\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF4a0pmZYUwr"
   },
   "source": [
    "## Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fH3zlobZUog"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcDAxVX7YQ31"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 1\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r7B7MC6YQ2I"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(feature_extractor, model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    feature_extractor.train()\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for sample, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.to(device)\n",
    "        batchsize = sample.shape[0]\n",
    "        fe_batch = []\n",
    "        for i in range(batchsize):\n",
    "            image_tensor = sample[i]\n",
    "            features = feature_extractor(image_tensor)\n",
    "            #print(features.shape)\n",
    "            features = concat_features(features)\n",
    "            features = features.view(3, 512, 160)\n",
    "            #print(features.shape)\n",
    "            fe_batch.append(features)\n",
    "\n",
    "        images = list(image.to(device) for image in fe_batch)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        #print(losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "DWvOavWsYQ0H",
    "outputId": "258b60d7-fd62-4a18-c86e-1f6cfd8d1aba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/126]  eta: 0:00:45  lr: 0.000045  loss: 3.8206 (3.8206)  loss_classifier: 0.1427 (0.1427)  loss_box_reg: 0.0051 (0.0051)  loss_objectness: 2.2652 (2.2652)  loss_rpn_box_reg: 1.4076 (1.4076)  time: 0.3627  data: 0.1629  max mem: 8165\n",
      "Epoch: [0]  [ 20/126]  eta: 0:00:19  lr: 0.000844  loss: 2.0000 (2.2709)  loss_classifier: 0.1150 (0.1568)  loss_box_reg: 0.0036 (0.0054)  loss_objectness: 0.5195 (0.9075)  loss_rpn_box_reg: 1.0376 (1.2011)  time: 0.1788  data: 0.0030  max mem: 8432\n",
      "Epoch: [0]  [ 40/126]  eta: 0:00:15  lr: 0.001643  loss: 1.3916 (1.9128)  loss_classifier: 0.1019 (0.1338)  loss_box_reg: 0.0013 (0.0037)  loss_objectness: 0.3839 (0.6369)  loss_rpn_box_reg: 0.9212 (1.1384)  time: 0.1789  data: 0.0033  max mem: 8432\n",
      "Epoch: [0]  [ 60/126]  eta: 0:00:11  lr: 0.002443  loss: 1.0471 (1.6605)  loss_classifier: 0.0957 (0.1286)  loss_box_reg: 0.0022 (0.0042)  loss_objectness: 0.1140 (0.4670)  loss_rpn_box_reg: 0.7875 (1.0607)  time: 0.1782  data: 0.0032  max mem: 8432\n",
      "Epoch: [0]  [ 80/126]  eta: 0:00:08  lr: 0.003242  loss: 1.0348 (1.5136)  loss_classifier: 0.0957 (0.1262)  loss_box_reg: 0.0029 (0.0047)  loss_objectness: 0.0886 (0.3746)  loss_rpn_box_reg: 0.8001 (1.0081)  time: 0.1790  data: 0.0033  max mem: 8432\n",
      "Epoch: [0]  [100/126]  eta: 0:00:04  lr: 0.004041  loss: 1.2868 (1.4716)  loss_classifier: 0.1066 (0.1246)  loss_box_reg: 0.0023 (0.0044)  loss_objectness: 0.0774 (0.3166)  loss_rpn_box_reg: 1.1002 (1.0261)  time: 0.1794  data: 0.0034  max mem: 8432\n",
      "Epoch: [0]  [120/126]  eta: 0:00:01  lr: 0.004840  loss: 1.0347 (1.4090)  loss_classifier: 0.1115 (0.1263)  loss_box_reg: 0.0032 (0.0052)  loss_objectness: 0.0687 (0.2765)  loss_rpn_box_reg: 0.8128 (1.0009)  time: 0.1773  data: 0.0031  max mem: 8432\n",
      "Epoch: [0]  [125/126]  eta: 0:00:00  lr: 0.005000  loss: 1.0370 (1.4296)  loss_classifier: 0.1022 (0.1262)  loss_box_reg: 0.0021 (0.0051)  loss_objectness: 0.0681 (0.2894)  loss_rpn_box_reg: 0.8419 (1.0088)  time: 0.1770  data: 0.0031  max mem: 8432\n",
      "Epoch: [0] Total time: 0:00:22 (0.1803 s / it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9401, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(feature_extractor, model, optimizer, test_loader, device, epoch, print_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NnX1worb7BY"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features)\n",
    "    features = features.view(3, 512, 160)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "oZH8AvMGePCh",
    "outputId": "e1e991fc-bfc1-4704-cd11-1b15c60c9e84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGrIApq0cR_2"
   },
   "outputs": [],
   "source": [
    "# model_test = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model_test = model_test.to(device)\n",
    "# model_test.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# x = list(image.to(device) for image in x)\n",
    "# predictions = model_test(x)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuHvpn_iaQpL"
   },
   "source": [
    "## Evaluate One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpX1eYADap3P"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(val_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFij7vtUYQx6"
   },
   "outputs": [],
   "source": [
    "def reorder_coord(pred_bboxes):\n",
    "    xmin, ymin, xmax, ymax = pred_bboxes.unbind(1)\n",
    "    return torch.stack((xmax, xmax, xmin, xmin, ymax, ymin, ymax, ymin), dim=1).view(-1, 2, 4)\n",
    "\n",
    "def get_bounding_boxes(samples):\n",
    "    # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n",
    "    # You need to return a tuple with size 'batch_size' and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of object\n",
    "\n",
    "    #Preparing inputs\n",
    "    batchsize = samples.shape[0]\n",
    "    fe_batch = []\n",
    "    for i in range(batchsize):\n",
    "        image_tensor = sample[i]\n",
    "        features = feature_extractor(image_tensor)\n",
    "        #print(features.shape)\n",
    "        features = concat_features(features)\n",
    "        features = features.view(3, 512, 160)\n",
    "        #print(features.shape)\n",
    "        fe_batch.append(features)\n",
    "\n",
    "    images = list(image.to(device) for image in fe_batch)\n",
    "    predictions = model(images)\n",
    "    res = []\n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i]\n",
    "        pred_bboxes = prediction['boxes']\n",
    "        reorder_pred_bboxes = reorder_coord(pred_bboxes)\n",
    "        res.append(reorder_pred_bboxes)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfZQ_U-waTAI"
   },
   "outputs": [],
   "source": [
    "def eval_one_epoch(feature_extractor, model, dataloader):\n",
    "    model.eval()\n",
    "    feature_extractor.eval()\n",
    "    total = 0\n",
    "    total_ats_bounding_boxes = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        total += 1\n",
    "        sample, target = data\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.cuda()\n",
    "\n",
    "        predicted_bounding_boxes = get_bounding_boxes(sample)[0].cpu()\n",
    "        \n",
    "\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes,\n",
    "                                                        target[0]['bounding_box'])\n",
    "        print('Number of pred bboxes {}'.format(predicted_bounding_boxes.shape))\n",
    "        print('ats_bounding_boxes {}'.format(ats_bounding_boxes))\n",
    "\n",
    "        total_ats_bounding_boxes += ats_bounding_boxes\n",
    "    return total_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMuwreKLaTF6"
   },
   "outputs": [],
   "source": [
    "total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE0B0pQ0e8FO"
   },
   "source": [
    "## Train and Eval for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXio30HjaS9-"
   },
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get feature extractor\n",
    "feature_extractor = torchvision.models.resnet18(pretrained=False)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2])\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 1\n",
    "epoch = 0\n",
    "print_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(feature_extractor, model, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "    best_model_wts = best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                                       'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                      }\n",
    "    losses = []\n",
    "    total_ats = []\n",
    "    best_total_ats = -1\n",
    "\n",
    "    loss = train_one_epoch(feature_extractor, model, optimizer, train_loader, device, epoch, print_freq)\n",
    "    total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, val_loader)\n",
    "    losses.append(loss)\n",
    "    total_ats.append(total_ats_bounding_boxes)\n",
    "    print('epoch {} loss {} total_ats {}'.format(epoch, loss, total_ats))\n",
    "\n",
    "    if total_ats_bounding_boxes > best_total_ats:\n",
    "        best_total_ats = total_ats_bounding_boxes\n",
    "        best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                           'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                 }\n",
    "        torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet18_0502_epoch{}.pth'.format(epoch))\n",
    "\n",
    "    return losses, total_ats, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPFR4eCVfF2e",
    "outputId": "e4181be0-88b1-479e-be4c-1f6481744565"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/126]  eta: 0:00:47  lr: 0.000045  loss: 3.5512 (3.5512)  loss_classifier: 0.2599 (0.2599)  loss_box_reg: 0.0063 (0.0063)  loss_objectness: 1.8867 (1.8867)  loss_rpn_box_reg: 1.3983 (1.3983)  time: 0.3789  data: 0.1737  max mem: 10063\n",
      "Epoch: [0]  [ 20/126]  eta: 0:00:19  lr: 0.000844  loss: 1.8488 (2.2739)  loss_classifier: 0.1346 (0.1544)  loss_box_reg: 0.0044 (0.0080)  loss_objectness: 0.5872 (0.9771)  loss_rpn_box_reg: 1.0572 (1.1344)  time: 0.1791  data: 0.0035  max mem: 10371\n",
      "Epoch: [0]  [ 40/126]  eta: 0:00:15  lr: 0.001643  loss: 1.3144 (1.7959)  loss_classifier: 0.1007 (0.1331)  loss_box_reg: 0.0015 (0.0057)  loss_objectness: 0.2158 (0.6163)  loss_rpn_box_reg: 0.9558 (1.0409)  time: 0.1784  data: 0.0033  max mem: 10371\n",
      "Epoch: [0]  [ 60/126]  eta: 0:00:12  lr: 0.002443  loss: 1.1813 (1.5778)  loss_classifier: 0.0880 (0.1252)  loss_box_reg: 0.0017 (0.0051)  loss_objectness: 0.1126 (0.4593)  loss_rpn_box_reg: 0.9283 (0.9881)  time: 0.1786  data: 0.0033  max mem: 10371\n",
      "Epoch: [0]  [ 80/126]  eta: 0:00:08  lr: 0.003242  loss: 1.0722 (1.4699)  loss_classifier: 0.1033 (0.1258)  loss_box_reg: 0.0013 (0.0052)  loss_objectness: 0.0947 (0.3753)  loss_rpn_box_reg: 0.8167 (0.9635)  time: 0.1780  data: 0.0031  max mem: 10371\n",
      "Epoch: [0]  [100/126]  eta: 0:00:04  lr: 0.004041  loss: 1.1018 (1.4116)  loss_classifier: 0.1104 (0.1264)  loss_box_reg: 0.0031 (0.0054)  loss_objectness: 0.0878 (0.3206)  loss_rpn_box_reg: 0.8615 (0.9593)  time: 0.1777  data: 0.0033  max mem: 10371\n",
      "Epoch: [0]  [120/126]  eta: 0:00:01  lr: 0.004840  loss: 1.0680 (1.3653)  loss_classifier: 0.0859 (0.1217)  loss_box_reg: 0.0010 (0.0048)  loss_objectness: 0.0813 (0.2818)  loss_rpn_box_reg: 0.9011 (0.9570)  time: 0.1769  data: 0.0033  max mem: 10371\n",
      "Epoch: [0]  [125/126]  eta: 0:00:00  lr: 0.005000  loss: 1.1182 (1.3561)  loss_classifier: 0.0859 (0.1217)  loss_box_reg: 0.0010 (0.0051)  loss_objectness: 0.0993 (0.2750)  loss_rpn_box_reg: 0.9034 (0.9543)  time: 0.1767  data: 0.0034  max mem: 10371\n",
      "Epoch: [0] Total time: 0:00:22 (0.1800 s / it)\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "Number of pred bboxes torch.Size([1, 2, 4])\n",
      "ats_bounding_boxes 0.0\n",
      "epoch 0 loss 1.1593073606491089 total_ats [tensor(0.)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-5dd300457fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbest_total_ats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_ats_bounding_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n\u001b[0;32m---> 18\u001b[0;31m                            \u001b[0;34m'decoder'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                                  }\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet18_0502_epoch{}.pth'.format(epoch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_block' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(train_losses, open('/scratch/nhl256/dl_project/model/object_detection_resnet18_0502_trainlosses.pickle', \"wb\"))\n",
    "pickle.dump(eval_total_ats, open('/scratch/nhl256/dl_project/model/object_detection_resnet18_0502_evalTotalAts.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sesiU-p20F"
   },
   "source": [
    "## Evaluate - IoU by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N7S-1srTbRg"
   },
   "outputs": [],
   "source": [
    "def prepare_pred_results(predictions):\n",
    "    pred_boxes = []\n",
    "    pred_labels = []\n",
    "    pred_scores = []\n",
    "    for prediction in predictions:\n",
    "        #print(prediction)\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        pred_boxes.append(boxes)\n",
    "        pred_labels.append(labels)\n",
    "        pred_scores.append(scores)\n",
    "\n",
    "    return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "def reorder_coord(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((ymin, xmin, ymax, xmax), dim=1)\n",
    "\n",
    "def prepare_gt(targets):\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        labels = target[\"labels\"].tolist()\n",
    "        gt_boxes.append(boxes)\n",
    "        gt_labels.append(labels)\n",
    "    return gt_boxes, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2a_EldORlxqm"
   },
   "outputs": [],
   "source": [
    "# Make sure that bbox_a, bbox_b = np.array\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    #print(type(bbox_a), type(bbox_b))\n",
    "    bbox_a = np.array(bbox_a)\n",
    "    bbox_b = np.array(bbox_b)\n",
    "\n",
    "    # print(type(bbox_a), type(bbox_b))\n",
    "    # print(bbox_a.shape, bbox_b.shape)\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # top left (i.e., ymin, xmin)\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right (i.e., ymax, xmax)\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    # Area of intersection: (tl < br) = bool, (br-tl) = (ymax-ymin) \n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahoOrX-8ZFzT"
   },
   "outputs": [],
   "source": [
    "def cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres=0.5):\n",
    "    iou_table = bbox_iou(pred_bbox_c, gt_bbox_c)\n",
    "    num_pred_bboxes = iou_table.shape[0]\n",
    "    num_gt_bboxes = iou_table.shape[1]\n",
    "    TP = np.zeros(num_pred_bboxes)\n",
    "    FP = np.zeros(num_pred_bboxes)\n",
    "    # For each pred_bounding box:\n",
    "      # Find the most relevant gt_bbox (i.e., the gt_bbox with max IoU)\n",
    "      # If IoU < threshold, then flag it as FP\n",
    "      # If IoU >= threshold, then:\n",
    "        # If that gt_bbox already has already matched with another pred_bbox:\n",
    "          # Flag it as FP\n",
    "        # Else:\n",
    "          # Flag it as TP\n",
    "\n",
    "    # TP only happens if the pred_bbox mathes with a gt_bbox\n",
    "    for i in range(num_pred_bboxes):\n",
    "        gt_bbox_index = np.argmax(iou_table[i])\n",
    "        best_pred_bbox_index_for_selected_gt_bbox = np.argmax(iou_table[:,gt_bbox_index])\n",
    "        if iou_table[i, gt_bbox_index] > iou_thres \\\n",
    "            and gt_bbox_index == best_pred_bbox_index_for_selected_gt_bbox:\n",
    "            TP[i] = 1\n",
    "        else:\n",
    "            FP[i] = 1\n",
    "\n",
    "    TP_cum = np.sum(TP)\n",
    "    FP_cum = np.sum(FP)\n",
    "\n",
    "    if (TP_cum + FP_cum) != num_pred_bboxes:\n",
    "        print(\"WRONG CALCULATION OF FP\")\n",
    "    return TP_cum, FP_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzx15kDKS3Zt"
   },
   "outputs": [],
   "source": [
    "# Test for cal_TP_FP_iou\n",
    "\n",
    "def inspect_call_TP_FP_iou(test_images, test_targets):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            # print(num_gt_bboxes)\n",
    "            # print(num_pred_bboxes)\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                print(class_TP + class_FP == num_pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8N2DCbcJe92u"
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print('Iter {}'.format(i))\n",
    "#     test_images, test_targets = next(iter(test_loader))\n",
    "#     inspect_call_TP_FP_iou(test_images, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "honU3_7QQLwr"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(predictions, test_targets, res, iou_thres=0.5):\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "    # res stores the TP_FP dict for each class\n",
    "    # Each TP_FP dict stores the TP_FP for each class \n",
    "    \n",
    "    batch_total_TP = 0\n",
    "    batch_total_FP = 0\n",
    "    batch_total_FN = 0\n",
    "    batch_total_num_object = 0\n",
    "    batch_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            #print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                #print(class_TP + class_FP == num_pred_bboxes)\n",
    "\n",
    "            batch_total_TP += class_TP\n",
    "            batch_total_FP += class_FP\n",
    "            batch_total_FN += class_FN\n",
    "            batch_total_num_object += num_gt_bboxes\n",
    "\n",
    "            batch_res[c]['TP'] += class_TP\n",
    "            batch_res[c]['FP'] += class_FP\n",
    "            batch_res[c]['FN'] += class_FN\n",
    "\n",
    "            res[c]['TP'] += class_TP\n",
    "            res[c]['FP'] += class_FP\n",
    "            res[c]['FN'] += class_FN\n",
    "            \n",
    "    return res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nji9KKjySsLT"
   },
   "outputs": [],
   "source": [
    "# Inspect evaluate_one_batch\n",
    "def inspect_evaluate_one_batch(test_images, test_targets, final_res):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "    return final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "qfzs9R0nS5Wr",
    "outputId": "bf329a68-8d6e-4aac-9662-391a10a96fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "batch 1\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 19}, 3: {'TP': 0, 'FP': 0, 'FN': 0}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 2}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 35}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "final_TP = 0\n",
    "final_FP = 0\n",
    "final_FN = 0\n",
    "final_num_objects = 0\n",
    "\n",
    "# test for 2 batches\n",
    "for i in range(2):\n",
    "    test_images, test_targets = next(iter(test_loader))\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = inspect_evaluate_one_batch(test_images, test_targets, final_res)\n",
    "\n",
    "    print('batch {}'.format(i))\n",
    "\n",
    "    \n",
    "    print(batch_total_TP, batch_total_FN, batch_total_num_object)\n",
    "    print('cur batch res:', batch_res)\n",
    "    print('final res after this batch:', final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hf3Cii6S5gf"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, iou_thres=0.5):\n",
    "    # Evaluate. for all data point in the evaluaton set\n",
    "    final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "    final_TP = 0\n",
    "    final_FP = 0\n",
    "    final_FN = 0\n",
    "    final_num_objects = 0\n",
    "\n",
    "    for iter_, (test_images, test_targets) in enumerate(test_loader):\n",
    "        # if iter_ % 50 == 0:\n",
    "        #     print('iter', iter_)\n",
    "        #print('iter', iter_)\n",
    "        test_images = torch.stack(test_images)\n",
    "        #print(test_images.shape)\n",
    "        test_images = prepare_inputs(test_images)\n",
    "        #print(test_images[0].shape)\n",
    "\n",
    "        test_images = list(image.to(device) for image in test_images)\n",
    "        test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "        model.eval()\n",
    "        predictions = model(test_images)\n",
    "\n",
    "        # Evaluate for one batch\n",
    "        final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "                    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "        \n",
    "        final_TP += batch_total_TP\n",
    "        final_FP += batch_total_FP\n",
    "        final_FN += batch_total_FN\n",
    "        final_num_objects += batch_total_num_object\n",
    "\n",
    "    return final_res, final_TP, final_FP, final_FN, final_num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Zop8Z75IS5ep",
    "outputId": "ed65157d-23c8-4f3f-8a91-00a27a6ee63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 50\n",
      "iter 100\n"
     ]
    }
   ],
   "source": [
    "final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFT09TgzS5cW"
   },
   "outputs": [],
   "source": [
    "def evaluate_threst_score(TP, FP, FN):\n",
    "    return (TP / (TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ejRrnjTloLBc",
    "outputId": "4c05fac2-7eb0-450a-b874-855a51e1b4e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       " 1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 2: {'FN': 3074, 'FP': 0, 'TP': 0},\n",
       " 3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       " 4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       " 5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       " 7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 8: {'FN': 0, 'FP': 0, 'TP': 0}}"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve5Zlqi-dvxf"
   },
   "source": [
    "## Train and Evaluate for Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_4-JrdeH59"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4fAgXgidudg"
   },
   "outputs": [],
   "source": [
    "def train_eval(model, train_loader, test_loader, iou_thres=0.5, num_epochs=10):\n",
    "    train_losses = []\n",
    "    eval_threatscores = []\n",
    "    eval_final_res = []\n",
    "    best_eval_ts = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(final_TP, final_FP, final_FN, final_num_objects)\n",
    "        eval_final_res.append(final_res)\n",
    "        eval_ts = evaluate_threst_score(final_TP, final_FP, final_FN)\n",
    "        eval_threatscores.append(eval_ts)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"epoch: {} eval_ts {}\".format(epoch, eval_ts))\n",
    "\n",
    "        if eval_ts > best_eval_ts:\n",
    "            best_eval_ts = eval_ts \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return model, best_model_wts, train_losses, eval_final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsAuhVThduhA"
   },
   "outputs": [],
   "source": [
    "model, best_model_wts, train_losses, eval_final_res = train_eval(model, train_loader,\n",
    "                                                                 test_loader, iou_thres=0.5,\n",
    "                                                                 num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "Naw6li0b09RY",
    "outputId": "70a1d53f-67ed-4e70-b5ad-3f8377c580ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3073.0, 'FP': 12599.0, 'TP': 1.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}},\n",
       " {0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3070.0, 'FP': 846.0, 'TP': 4.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}}]"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llVJ0SeDM40i"
   },
   "source": [
    "## Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xzLLZktM4aI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7w8Da6JM8or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXslpd4-d-rQ"
   },
   "source": [
    "## Customize Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGCMc2Ppd-rR"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IC8olie21GE"
   },
   "source": [
    "#### 1. Mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "cukXGxWk1WtO",
    "outputId": "1c2db8fe-1068-4235-9348-d70fcb3ea02b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47b1e48fc071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n\u001b[1;32m      4\u001b[0m                                     aspect_ratios=((0.5, 1.0, 2.0),))\n\u001b[1;32m      5\u001b[0m roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                    num_classes=21,\n",
    "                    rpn_anchor_generator=anchor_generator,\n",
    "                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVBYaALa2xOK"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7XYTwLa26Mk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmOGwmnO26uZ"
   },
   "source": [
    "#### 2. CustomVGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIB1q2yd-rl"
   },
   "outputs": [],
   "source": [
    "def customize_VGG16():\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    \n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "    \n",
    "    classifier = list(classifier)\n",
    "    # delete the Linear layer\n",
    "    del classifier[6]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    #freeze top4 conv layer\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "    features = nn.Sequential(*features)\n",
    "        \n",
    "    return features, classifier\n",
    "backbone, box_head = customize_VGG16()\n",
    "backbone.out_channels = 512\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pb3n-b1_pNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "objectDectionFastRCNN_050220.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
