{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "LMpEF8ZDd-lt",
    "outputId": "d4563cbf-c762-48a9-dbb7-6cb0ecc0c4fd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "os.chdir('/scratch/nhl256/dl_project/code/')\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import *\n",
    "from nhung_data_helper import FastRCNNLabeledDataset\n",
    "from helper import *\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGxs-4igvAzk"
   },
   "source": [
    "### 05/04/20\n",
    "\n",
    "- Concat 6 images along 1 dim before passing through the model\n",
    "- Saving the feature extractor and the fasterRCNN model\n",
    "- ResNet50 for feature extraction using the weights from self-supervised task (/scratch/bva212/dl_project/jigsaw_best_model_fe.pth)\n",
    "- optimizer = torch.optim.Adam(params, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "X9PMrKKreF3N",
    "outputId": "38430281-d6fc-4ab0-d551-3da75203b517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "\n",
    "# image_folder = '../data'\n",
    "# annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "image_folder = 'data/data'\n",
    "annotation_csv = 'data/data/annotation.csv'\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# image_folder = '/Users/nhungle/Downloads/dl20_data'\n",
    "# annotation_csv = '/Users/nhungle/Downloads/dl20_data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vmrTQbev_exi",
    "outputId": "eb026039-9b09-452f-e116-efaa4274ee56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htb4imOktWfj"
   },
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "random.seed(1008)\n",
    "random.shuffle(labeled_scene_index)\n",
    "train_labeled_scene_index = labeled_scene_index[:22]\n",
    "val_labeled_scene_index = labeled_scene_index[22:26]\n",
    "test_labeled_scene_index = labeled_scene_index[26:]\n",
    "BATCH_SIZE=1\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "fasterRCNN_trainset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "train_loader = torch.utils.data.DataLoader(fasterRCNN_trainset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_valset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "val_loader = torch.utils.data.DataLoader(fasterRCNN_valset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "fasterRCNN_testset = FastRCNNLabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "test_loader = torch.utils.data.DataLoader(fasterRCNN_testset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gDRi-fS1TAlH",
    "outputId": "724493de-d954-425f-c0e9-09abdcaf4960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLuS_ttrTT_j"
   },
   "source": [
    "### Get Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = torchvision.models.resnet50(pretrained=False)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2])\n",
    "feature_extractor.load_state_dict(torch.load('/scratch/bva212/dl_project/jigsaw_best_model_fe.pth'))\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVI7nFEITWEw"
   },
   "outputs": [],
   "source": [
    "def concat_features(features, dim = 2):\n",
    "    #dim 0 ==> stacking the images in the channel dimension\n",
    "    #dim 1 ==> stacking the images in row dimension\n",
    "    #dim 2 ==> stacking the images in column dimension\n",
    "    tensor_tuples = torch.unbind(features, dim=0)\n",
    "    concatenated_fm = torch.cat(tensor_tuples, dim=dim)\n",
    "    return concatenated_fm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrTHbwweW-4B"
   },
   "source": [
    "### Inspect if it works on a pretrained FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HwKa9R3TXInF",
    "outputId": "35be123d-30e1-4db2-cedc-53da22551da2"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features)\n",
    "    features = features.view(3, 2048, 160)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "CnnOF2RsT1j_",
    "outputId": "0eee6720-3222-45fe-def7-f737ad7976c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 'loss_box_reg': tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(4.5261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(1.9608, device='cuda:0', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "output1 = model(images, targets)\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF4a0pmZYUwr"
   },
   "source": [
    "## Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fH3zlobZUog"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcDAxVX7YQ31"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 1\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r7B7MC6YQ2I"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(feature_extractor, model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    feature_extractor.train()\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for sample, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.to(device)\n",
    "        batchsize = sample.shape[0]\n",
    "        fe_batch = []\n",
    "        for i in range(batchsize):\n",
    "            image_tensor = sample[i]\n",
    "            features = feature_extractor(image_tensor)\n",
    "            #print(features.shape)\n",
    "            features = concat_features(features)\n",
    "            features = features.view(3, 2048, 160)\n",
    "            #print(features.shape)\n",
    "            fe_batch.append(features)\n",
    "\n",
    "        images = list(image.to(device) for image in fe_batch)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        #print(losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DWvOavWsYQ0H",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "258b60d7-fd62-4a18-c86e-1f6cfd8d1aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/252]  eta: 0:04:00  lr: 0.000000  loss: 8.4684 (8.4684)  loss_classifier: 0.2124 (0.2124)  loss_box_reg: 0.0096 (0.0096)  loss_objectness: 5.2578 (5.2578)  loss_rpn_box_reg: 2.9887 (2.9887)  time: 0.9557  data: 0.2916  max mem: 3323\n",
      "Epoch: [0]  [ 20/252]  eta: 0:00:54  lr: 0.000008  loss: 4.9731 (5.2540)  loss_classifier: 0.1493 (0.1541)  loss_box_reg: 0.0048 (0.0063)  loss_objectness: 2.8656 (3.1229)  loss_rpn_box_reg: 1.7637 (1.9708)  time: 0.2006  data: 0.0038  max mem: 3889\n",
      "Epoch: [0]  [ 40/252]  eta: 0:00:46  lr: 0.000016  loss: 1.0549 (3.3331)  loss_classifier: 0.0761 (0.1229)  loss_box_reg: 0.0018 (0.0043)  loss_objectness: 0.1014 (1.6905)  loss_rpn_box_reg: 0.8280 (1.5153)  time: 0.2015  data: 0.0042  max mem: 3889\n",
      "Epoch: [0]  [ 60/252]  eta: 0:00:40  lr: 0.000024  loss: 0.8383 (2.5197)  loss_classifier: 0.0880 (0.1098)  loss_box_reg: 0.0014 (0.0035)  loss_objectness: 0.0224 (1.1616)  loss_rpn_box_reg: 0.7036 (1.2448)  time: 0.1957  data: 0.0036  max mem: 3889\n",
      "Epoch: [0]  [ 80/252]  eta: 0:00:35  lr: 0.000032  loss: 0.7512 (2.1051)  loss_classifier: 0.0982 (0.1052)  loss_box_reg: 0.0009 (0.0034)  loss_objectness: 0.0208 (0.8810)  loss_rpn_box_reg: 0.6129 (1.1156)  time: 0.1990  data: 0.0039  max mem: 3889\n",
      "Epoch: [0]  [100/252]  eta: 0:00:31  lr: 0.000040  loss: 0.6563 (1.8269)  loss_classifier: 0.0461 (0.0978)  loss_box_reg: 0.0006 (0.0029)  loss_objectness: 0.0253 (0.7118)  loss_rpn_box_reg: 0.5110 (1.0144)  time: 0.1963  data: 0.0040  max mem: 3889\n",
      "Epoch: [0]  [120/252]  eta: 0:00:27  lr: 0.000048  loss: 0.7738 (1.6594)  loss_classifier: 0.0839 (0.0959)  loss_box_reg: 0.0006 (0.0027)  loss_objectness: 0.0116 (0.5976)  loss_rpn_box_reg: 0.6734 (0.9633)  time: 0.1987  data: 0.0039  max mem: 3889\n",
      "Epoch: [0]  [140/252]  eta: 0:00:22  lr: 0.000056  loss: 0.7742 (1.5304)  loss_classifier: 0.0845 (0.0933)  loss_box_reg: 0.0005 (0.0025)  loss_objectness: 0.0268 (0.5166)  loss_rpn_box_reg: 0.6645 (0.9179)  time: 0.1933  data: 0.0034  max mem: 3889\n",
      "Epoch: [0]  [160/252]  eta: 0:00:18  lr: 0.000064  loss: 0.7288 (1.4296)  loss_classifier: 0.0846 (0.0920)  loss_box_reg: 0.0003 (0.0023)  loss_objectness: 0.0114 (0.4549)  loss_rpn_box_reg: 0.6221 (0.8805)  time: 0.1965  data: 0.0036  max mem: 3889\n",
      "Epoch: [0]  [180/252]  eta: 0:00:14  lr: 0.000072  loss: 0.3731 (1.3231)  loss_classifier: 0.0380 (0.0875)  loss_box_reg: 0.0003 (0.0022)  loss_objectness: 0.0098 (0.4062)  loss_rpn_box_reg: 0.3313 (0.8272)  time: 0.1948  data: 0.0037  max mem: 3889\n",
      "Epoch: [0]  [200/252]  eta: 0:00:10  lr: 0.000080  loss: 0.6082 (1.2562)  loss_classifier: 0.0788 (0.0858)  loss_box_reg: 0.0005 (0.0022)  loss_objectness: 0.0137 (0.3679)  loss_rpn_box_reg: 0.4941 (0.8003)  time: 0.1958  data: 0.0036  max mem: 3889\n",
      "Epoch: [0]  [220/252]  eta: 0:00:06  lr: 0.000088  loss: 0.6191 (1.1986)  loss_classifier: 0.0495 (0.0834)  loss_box_reg: 0.0001 (0.0022)  loss_objectness: 0.0081 (0.3362)  loss_rpn_box_reg: 0.5075 (0.7768)  time: 0.1965  data: 0.0036  max mem: 3889\n",
      "Epoch: [0]  [240/252]  eta: 0:00:02  lr: 0.000096  loss: 0.5507 (1.1419)  loss_classifier: 0.0555 (0.0813)  loss_box_reg: 0.0004 (0.0021)  loss_objectness: 0.0153 (0.3099)  loss_rpn_box_reg: 0.4741 (0.7486)  time: 0.2014  data: 0.0038  max mem: 3889\n",
      "Epoch: [0]  [251/252]  eta: 0:00:00  lr: 0.000100  loss: 0.5679 (1.1194)  loss_classifier: 0.0642 (0.0803)  loss_box_reg: 0.0004 (0.0021)  loss_objectness: 0.0145 (0.2969)  loss_rpn_box_reg: 0.4833 (0.7401)  time: 0.1980  data: 0.0037  max mem: 3889\n",
      "Epoch: [0] Total time: 0:00:50 (0.2008 s / it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5374, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(feature_extractor, model, optimizer, test_loader, device, epoch, print_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NnX1worb7BY"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(train_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)\n",
    "batchsize = sample.shape[0]\n",
    "fe_batch = []\n",
    "for i in range(batchsize):\n",
    "    image_tensor = sample[i]\n",
    "    features = feature_extractor(image_tensor)\n",
    "    #print(features.shape)\n",
    "    features = concat_features(features)\n",
    "    features = features.view(3, 2048, 160)\n",
    "    #print(features.shape)\n",
    "    fe_batch.append(features)\n",
    "\n",
    "images = list(image.to(device) for image in fe_batch)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oZH8AvMGePCh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e1e991fc-bfc1-4704-cd11-1b15c60c9e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 2.5249, 12.3105,  8.4614, 16.3655],\n",
      "        [ 2.8026, 15.6608,  8.3191, 19.5138],\n",
      "        [ 0.2205, 11.7770, 11.2676, 21.1414],\n",
      "        [ 2.3730, 10.1673,  8.3150, 14.2311],\n",
      "        [ 4.2386, 17.2535, 10.2578, 21.1323],\n",
      "        [ 4.0106, 21.9735,  9.0826, 25.4802],\n",
      "        [ 4.5081, 31.6906,  9.4630, 34.7576],\n",
      "        [ 4.6023, 27.9308,  9.6325, 31.2239],\n",
      "        [ 5.4734, 15.9906, 11.2388, 19.9430],\n",
      "        [ 5.5933, 32.4770, 10.3822, 35.6822],\n",
      "        [ 2.7287, 15.6712,  8.3585, 19.4111],\n",
      "        [ 4.1645, 17.2663, 10.3018, 21.0387],\n",
      "        [ 2.8582,  7.2621,  8.4121, 10.6865],\n",
      "        [ 3.9470, 21.9863,  9.1202, 25.3956],\n",
      "        [ 5.4094, 16.0035, 11.2775, 19.8541],\n",
      "        [ 4.8536, 23.1879, 10.6860, 27.1027],\n",
      "        [ 2.4670, 16.6693,  8.8344, 20.6607],\n",
      "        [ 0.2549, 10.0921, 16.7064, 26.0459],\n",
      "        [ 6.9108, 26.4959, 12.0508, 30.1776],\n",
      "        [ 2.4453, 12.3143,  8.4999, 16.2561],\n",
      "        [ 6.5598, 24.2177, 12.2212, 27.9088]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 2, 1, 3, 3, 3],\n",
      "       device='cuda:0'), 'scores': tensor([0.0866, 0.0837, 0.0825, 0.0796, 0.0658, 0.0576, 0.0573, 0.0564, 0.0560,\n",
      "        0.0552, 0.0549, 0.0545, 0.0540, 0.0540, 0.0539, 0.0537, 0.0526, 0.0516,\n",
      "        0.0512, 0.0508, 0.0507], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGrIApq0cR_2"
   },
   "outputs": [],
   "source": [
    "# model_test = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model_test = model_test.to(device)\n",
    "# model_test.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# x = list(image.to(device) for image in x)\n",
    "# predictions = model_test(x)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuHvpn_iaQpL"
   },
   "source": [
    "## Evaluate One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpX1eYADap3P"
   },
   "outputs": [],
   "source": [
    "sample, targets = next(iter(val_loader))\n",
    "sample = torch.stack(sample)\n",
    "sample = sample.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFij7vtUYQx6"
   },
   "outputs": [],
   "source": [
    "def reorder_coord(pred_bboxes):\n",
    "    xmin, ymin, xmax, ymax = pred_bboxes.unbind(1)\n",
    "    return torch.stack((xmax, xmax, xmin, xmin, ymax, ymin, ymax, ymin), dim=1).view(-1, 2, 4)\n",
    "\n",
    "def get_bounding_boxes(samples):\n",
    "    # samples is a cuda tensor with size [batch_size, 6, 3, 256, 306]\n",
    "    # You need to return a tuple with size 'batch_size' and each element is a cuda tensor [N, 2, 4]\n",
    "    # where N is the number of object\n",
    "\n",
    "    #Preparing inputs\n",
    "    batchsize = samples.shape[0]\n",
    "    fe_batch = []\n",
    "    for i in range(batchsize):\n",
    "        image_tensor = sample[i]\n",
    "        features = feature_extractor(image_tensor)\n",
    "        #print(features.shape)\n",
    "        features = concat_features(features)\n",
    "        features = features.view(3, 2048, 160)\n",
    "        #print(features.shape)\n",
    "        fe_batch.append(features)\n",
    "\n",
    "    images = list(image.to(device) for image in fe_batch)\n",
    "    predictions = model(images)\n",
    "    res = []\n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i]\n",
    "        pred_bboxes = prediction['boxes']\n",
    "        reorder_pred_bboxes = reorder_coord(pred_bboxes)\n",
    "        res.append(reorder_pred_bboxes)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfZQ_U-waTAI"
   },
   "outputs": [],
   "source": [
    "def eval_one_epoch(feature_extractor, model, dataloader):\n",
    "    model.eval()\n",
    "    feature_extractor.eval()\n",
    "    total = 0\n",
    "    total_ats_bounding_boxes = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        total += 1\n",
    "        sample, target = data\n",
    "        sample = torch.stack(sample)\n",
    "        sample = sample.cuda()\n",
    "\n",
    "        predicted_bounding_boxes = get_bounding_boxes(sample)[0].cpu()\n",
    "        \n",
    "\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes,\n",
    "                                                        target[0]['bounding_box'])\n",
    "#         print('Number of pred bboxes {}'.format(predicted_bounding_boxes.shape))\n",
    "#         print('ats_bounding_boxes {}'.format(ats_bounding_boxes))\n",
    "\n",
    "        total_ats_bounding_boxes += ats_bounding_boxes\n",
    "    return total_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMuwreKLaTF6"
   },
   "outputs": [],
   "source": [
    "total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0043)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ats_bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE0B0pQ0e8FO"
   },
   "source": [
    "## Train and Eval for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXio30HjaS9-"
   },
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get feature extractor\n",
    "feature_extractor = torchvision.models.resnet50(pretrained=False)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2])\n",
    "feature_extractor.load_state_dict(torch.load('/scratch/bva212/dl_project/jigsaw_best_model_fe.pth'))\n",
    "feature_extractor.to(device)\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(feature_extractor, model, train_loader, val_loader, num_epochs=10):\n",
    "    best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                       'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                      }\n",
    "    losses = []\n",
    "    total_ats = []\n",
    "    best_total_ats = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(feature_extractor, model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        total_ats_bounding_boxes = eval_one_epoch(feature_extractor, model, val_loader)\n",
    "        losses.append(loss)\n",
    "        total_ats.append(total_ats_bounding_boxes)\n",
    "        print('epoch {} loss {} total_ats {}'.format(epoch, loss, total_ats))\n",
    "\n",
    "        if total_ats_bounding_boxes > best_total_ats:\n",
    "            best_total_ats = total_ats_bounding_boxes\n",
    "            best_model_wts = {'feature_extractor': copy.deepcopy(feature_extractor.state_dict()),\n",
    "                               'fasterRCNN': copy.deepcopy(model.state_dict())\n",
    "                                     }\n",
    "            torch.save(best_model_wts, '/scratch/nhl256/dl_project/model/object_detection_resnet50_0504_epoch{}.pth'.format(epoch))\n",
    "\n",
    "    return losses, total_ats, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2772]  eta: 0:31:18  lr: 0.000000  loss: 8.1537 (8.1537)  loss_classifier: 0.4193 (0.4193)  loss_box_reg: 0.0219 (0.0219)  loss_objectness: 5.4766 (5.4766)  loss_rpn_box_reg: 2.2359 (2.2359)  time: 0.6775  data: 0.2160  max mem: 6691\n",
      "Epoch: [0]  [ 500/2772]  eta: 0:07:35  lr: 0.000050  loss: 0.6420 (1.1176)  loss_classifier: 0.0852 (0.1452)  loss_box_reg: 0.0005 (0.0052)  loss_objectness: 0.0040 (0.2946)  loss_rpn_box_reg: 0.5094 (0.6726)  time: 0.1987  data: 0.0038  max mem: 6691\n",
      "Epoch: [0]  [1000/2772]  eta: 0:05:52  lr: 0.000100  loss: 0.5096 (0.8550)  loss_classifier: 0.0404 (0.1076)  loss_box_reg: 0.0001 (0.0046)  loss_objectness: 0.0048 (0.1535)  loss_rpn_box_reg: 0.4521 (0.5893)  time: 0.1979  data: 0.0038  max mem: 6691\n",
      "Epoch: [0]  [1500/2772]  eta: 0:04:12  lr: 0.000100  loss: 0.4705 (0.7575)  loss_classifier: 0.0387 (0.0901)  loss_box_reg: 0.0001 (0.0045)  loss_objectness: 0.0080 (0.1058)  loss_rpn_box_reg: 0.3839 (0.5571)  time: 0.1974  data: 0.0038  max mem: 6691\n",
      "Epoch: [0]  [2000/2772]  eta: 0:02:32  lr: 0.000100  loss: 0.5019 (0.7040)  loss_classifier: 0.0523 (0.0829)  loss_box_reg: 0.0004 (0.0047)  loss_objectness: 0.0046 (0.0819)  loss_rpn_box_reg: 0.4123 (0.5346)  time: 0.2000  data: 0.0044  max mem: 6691\n",
      "Epoch: [0]  [2500/2772]  eta: 0:00:53  lr: 0.000100  loss: 0.5875 (0.6784)  loss_classifier: 0.0488 (0.0779)  loss_box_reg: 0.0004 (0.0047)  loss_objectness: 0.0021 (0.0674)  loss_rpn_box_reg: 0.4587 (0.5284)  time: 0.1973  data: 0.0038  max mem: 6691\n",
      "Epoch: [0]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5269 (0.6651)  loss_classifier: 0.0646 (0.0757)  loss_box_reg: 0.0029 (0.0047)  loss_objectness: 0.0029 (0.0617)  loss_rpn_box_reg: 0.4343 (0.5231)  time: 0.1976  data: 0.0040  max mem: 6691\n",
      "Epoch: [0] Total time: 0:09:07 (0.1977 s / it)\n",
      "epoch 0 loss 0.34904903173446655 total_ats [tensor(0.0394)]\n",
      "Epoch: [1]  [   0/2772]  eta: 0:25:54  lr: 0.000100  loss: 0.4386 (0.4386)  loss_classifier: 0.0368 (0.0368)  loss_box_reg: 0.0045 (0.0045)  loss_objectness: 0.0036 (0.0036)  loss_rpn_box_reg: 0.3937 (0.3937)  time: 0.5607  data: 0.3348  max mem: 7460\n",
      "Epoch: [1]  [ 500/2772]  eta: 0:07:34  lr: 0.000100  loss: 0.4320 (0.5514)  loss_classifier: 0.0422 (0.0570)  loss_box_reg: 0.0030 (0.0047)  loss_objectness: 0.0024 (0.0091)  loss_rpn_box_reg: 0.3884 (0.4805)  time: 0.1996  data: 0.0043  max mem: 7460\n",
      "Epoch: [1]  [1000/2772]  eta: 0:05:52  lr: 0.000100  loss: 0.4583 (0.5373)  loss_classifier: 0.0487 (0.0580)  loss_box_reg: 0.0046 (0.0053)  loss_objectness: 0.0030 (0.0092)  loss_rpn_box_reg: 0.3779 (0.4648)  time: 0.1979  data: 0.0039  max mem: 7460\n",
      "Epoch: [1]  [1500/2772]  eta: 0:04:12  lr: 0.000100  loss: 0.5084 (0.5467)  loss_classifier: 0.0423 (0.0592)  loss_box_reg: 0.0002 (0.0051)  loss_objectness: 0.0005 (0.0092)  loss_rpn_box_reg: 0.4304 (0.4732)  time: 0.1949  data: 0.0037  max mem: 7460\n",
      "Epoch: [1]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.5047 (0.5476)  loss_classifier: 0.0545 (0.0591)  loss_box_reg: 0.0069 (0.0050)  loss_objectness: 0.0022 (0.0092)  loss_rpn_box_reg: 0.4447 (0.4743)  time: 0.1994  data: 0.0042  max mem: 7460\n",
      "Epoch: [1]  [2500/2772]  eta: 0:00:53  lr: 0.000100  loss: 0.5094 (0.5429)  loss_classifier: 0.0645 (0.0578)  loss_box_reg: 0.0021 (0.0049)  loss_objectness: 0.0039 (0.0092)  loss_rpn_box_reg: 0.4403 (0.4710)  time: 0.1966  data: 0.0039  max mem: 7460\n",
      "Epoch: [1]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5119 (0.5409)  loss_classifier: 0.0668 (0.0579)  loss_box_reg: 0.0073 (0.0049)  loss_objectness: 0.0039 (0.0099)  loss_rpn_box_reg: 0.4020 (0.4682)  time: 0.1976  data: 0.0041  max mem: 7460\n",
      "Epoch: [1] Total time: 0:09:09 (0.1982 s / it)\n",
      "epoch 1 loss 0.5264302492141724 total_ats [tensor(0.0394), tensor(0.)]\n",
      "Epoch: [2]  [   0/2772]  eta: 0:22:12  lr: 0.000100  loss: 0.3959 (0.3959)  loss_classifier: 0.0392 (0.0392)  loss_box_reg: 0.0003 (0.0003)  loss_objectness: 0.0144 (0.0144)  loss_rpn_box_reg: 0.3419 (0.3419)  time: 0.4809  data: 0.2778  max mem: 7464\n",
      "Epoch: [2]  [ 500/2772]  eta: 0:07:33  lr: 0.000100  loss: 0.4987 (0.5306)  loss_classifier: 0.0399 (0.0544)  loss_box_reg: 0.0001 (0.0049)  loss_objectness: 0.0023 (0.0094)  loss_rpn_box_reg: 0.4310 (0.4618)  time: 0.1981  data: 0.0039  max mem: 7464\n",
      "Epoch: [2]  [1000/2772]  eta: 0:05:53  lr: 0.000100  loss: 0.4892 (0.5337)  loss_classifier: 0.0396 (0.0555)  loss_box_reg: 0.0003 (0.0050)  loss_objectness: 0.0032 (0.0087)  loss_rpn_box_reg: 0.4094 (0.4645)  time: 0.1984  data: 0.0040  max mem: 7464\n",
      "Epoch: [2]  [1500/2772]  eta: 0:04:13  lr: 0.000100  loss: 0.4624 (0.5279)  loss_classifier: 0.0376 (0.0552)  loss_box_reg: 0.0004 (0.0049)  loss_objectness: 0.0017 (0.0088)  loss_rpn_box_reg: 0.4112 (0.4590)  time: 0.1981  data: 0.0040  max mem: 7464\n",
      "Epoch: [2]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.3963 (0.5207)  loss_classifier: 0.0406 (0.0547)  loss_box_reg: 0.0042 (0.0051)  loss_objectness: 0.0020 (0.0089)  loss_rpn_box_reg: 0.3274 (0.4520)  time: 0.1984  data: 0.0038  max mem: 7464\n",
      "Epoch: [2]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4636 (0.5187)  loss_classifier: 0.0615 (0.0544)  loss_box_reg: 0.0096 (0.0052)  loss_objectness: 0.0059 (0.0090)  loss_rpn_box_reg: 0.3696 (0.4501)  time: 0.1971  data: 0.0038  max mem: 7464\n",
      "Epoch: [2]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5219 (0.5179)  loss_classifier: 0.0626 (0.0544)  loss_box_reg: 0.0042 (0.0052)  loss_objectness: 0.0029 (0.0091)  loss_rpn_box_reg: 0.4300 (0.4492)  time: 0.1954  data: 0.0038  max mem: 7464\n",
      "Epoch: [2] Total time: 0:09:10 (0.1985 s / it)\n",
      "epoch 2 loss 0.6727716326713562 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118)]\n",
      "Epoch: [3]  [   0/2772]  eta: 0:22:35  lr: 0.000100  loss: 0.5155 (0.5155)  loss_classifier: 0.0274 (0.0274)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.4869 (0.4869)  time: 0.4891  data: 0.2595  max mem: 7464\n",
      "Epoch: [3]  [ 500/2772]  eta: 0:07:32  lr: 0.000100  loss: 0.5016 (0.5125)  loss_classifier: 0.0637 (0.0531)  loss_box_reg: 0.0074 (0.0052)  loss_objectness: 0.0029 (0.0084)  loss_rpn_box_reg: 0.4210 (0.4458)  time: 0.1981  data: 0.0040  max mem: 7464\n",
      "Epoch: [3]  [1000/2772]  eta: 0:05:54  lr: 0.000100  loss: 0.3721 (0.5059)  loss_classifier: 0.0338 (0.0535)  loss_box_reg: 0.0023 (0.0054)  loss_objectness: 0.0055 (0.0084)  loss_rpn_box_reg: 0.3249 (0.4387)  time: 0.2007  data: 0.0041  max mem: 7464\n",
      "Epoch: [3]  [1500/2772]  eta: 0:04:13  lr: 0.000100  loss: 0.5191 (0.5042)  loss_classifier: 0.0429 (0.0532)  loss_box_reg: 0.0005 (0.0053)  loss_objectness: 0.0015 (0.0083)  loss_rpn_box_reg: 0.4165 (0.4375)  time: 0.1978  data: 0.0040  max mem: 7464\n",
      "Epoch: [3]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.4243 (0.5031)  loss_classifier: 0.0311 (0.0532)  loss_box_reg: 0.0002 (0.0053)  loss_objectness: 0.0026 (0.0084)  loss_rpn_box_reg: 0.3557 (0.4362)  time: 0.1977  data: 0.0039  max mem: 7464\n",
      "Epoch: [3]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4143 (0.5005)  loss_classifier: 0.0521 (0.0535)  loss_box_reg: 0.0047 (0.0054)  loss_objectness: 0.0018 (0.0082)  loss_rpn_box_reg: 0.3578 (0.4334)  time: 0.2007  data: 0.0044  max mem: 7464\n",
      "epoch 3 loss 0.14119109511375427 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.)]\n",
      "Epoch: [4]  [   0/2772]  eta: 0:19:50  lr: 0.000100  loss: 0.1582 (0.1582)  loss_classifier: 0.0184 (0.0184)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.0008 (0.0008)  loss_rpn_box_reg: 0.1390 (0.1390)  time: 0.4294  data: 0.2137  max mem: 7464\n",
      "Epoch: [4]  [ 500/2772]  eta: 0:07:32  lr: 0.000100  loss: 0.5201 (0.5024)  loss_classifier: 0.0376 (0.0515)  loss_box_reg: 0.0027 (0.0053)  loss_objectness: 0.0032 (0.0081)  loss_rpn_box_reg: 0.4081 (0.4375)  time: 0.1982  data: 0.0040  max mem: 7464\n",
      "Epoch: [4]  [1000/2772]  eta: 0:05:53  lr: 0.000100  loss: 0.4265 (0.4972)  loss_classifier: 0.0496 (0.0511)  loss_box_reg: 0.0049 (0.0054)  loss_objectness: 0.0017 (0.0080)  loss_rpn_box_reg: 0.3880 (0.4327)  time: 0.1983  data: 0.0040  max mem: 7464\n",
      "Epoch: [4]  [1500/2772]  eta: 0:04:13  lr: 0.000100  loss: 0.4420 (0.4905)  loss_classifier: 0.0454 (0.0507)  loss_box_reg: 0.0002 (0.0054)  loss_objectness: 0.0042 (0.0076)  loss_rpn_box_reg: 0.3913 (0.4268)  time: 0.1959  data: 0.0038  max mem: 7464\n",
      "Epoch: [4]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.3545 (0.4880)  loss_classifier: 0.0420 (0.0510)  loss_box_reg: 0.0042 (0.0054)  loss_objectness: 0.0057 (0.0080)  loss_rpn_box_reg: 0.3012 (0.4235)  time: 0.1995  data: 0.0040  max mem: 7464\n",
      "Epoch: [4]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4475 (0.4867)  loss_classifier: 0.0388 (0.0508)  loss_box_reg: 0.0002 (0.0054)  loss_objectness: 0.0023 (0.0081)  loss_rpn_box_reg: 0.3537 (0.4223)  time: 0.1979  data: 0.0042  max mem: 7464\n",
      "Epoch: [4]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.4621 (0.4884)  loss_classifier: 0.0475 (0.0511)  loss_box_reg: 0.0036 (0.0055)  loss_objectness: 0.0018 (0.0080)  loss_rpn_box_reg: 0.3983 (0.4238)  time: 0.1996  data: 0.0043  max mem: 7464\n",
      "Epoch: [4] Total time: 0:09:11 (0.1991 s / it)\n",
      "epoch 4 loss 0.6785928010940552 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413)]\n",
      "Epoch: [5]  [   0/2772]  eta: 1:06:17  lr: 0.000100  loss: 0.5207 (0.5207)  loss_classifier: 0.0563 (0.0563)  loss_box_reg: 0.0122 (0.0122)  loss_objectness: 0.0017 (0.0017)  loss_rpn_box_reg: 0.4505 (0.4505)  time: 1.4347  data: 1.2145  max mem: 7464\n",
      "Epoch: [5]  [ 500/2772]  eta: 0:07:38  lr: 0.000100  loss: 0.4283 (0.4900)  loss_classifier: 0.0448 (0.0511)  loss_box_reg: 0.0060 (0.0061)  loss_objectness: 0.0038 (0.0080)  loss_rpn_box_reg: 0.3695 (0.4248)  time: 0.1992  data: 0.0042  max mem: 7464\n",
      "Epoch: [5]  [1000/2772]  eta: 0:05:54  lr: 0.000100  loss: 0.4320 (0.4734)  loss_classifier: 0.0429 (0.0500)  loss_box_reg: 0.0048 (0.0060)  loss_objectness: 0.0026 (0.0081)  loss_rpn_box_reg: 0.3638 (0.4094)  time: 0.1979  data: 0.0039  max mem: 7464\n",
      "Epoch: [5]  [1500/2772]  eta: 0:04:14  lr: 0.000100  loss: 0.5956 (0.4702)  loss_classifier: 0.0376 (0.0505)  loss_box_reg: 0.0020 (0.0061)  loss_objectness: 0.0024 (0.0079)  loss_rpn_box_reg: 0.4914 (0.4057)  time: 0.1997  data: 0.0045  max mem: 7464\n",
      "Epoch: [5]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.5949 (0.4705)  loss_classifier: 0.0467 (0.0504)  loss_box_reg: 0.0032 (0.0060)  loss_objectness: 0.0014 (0.0078)  loss_rpn_box_reg: 0.5334 (0.4063)  time: 0.1993  data: 0.0042  max mem: 7464\n",
      "Epoch: [5]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.3570 (0.4712)  loss_classifier: 0.0402 (0.0501)  loss_box_reg: 0.0029 (0.0058)  loss_objectness: 0.0016 (0.0080)  loss_rpn_box_reg: 0.3193 (0.4073)  time: 0.1981  data: 0.0040  max mem: 7464\n",
      "Epoch: [5]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.5663 (0.4741)  loss_classifier: 0.0615 (0.0506)  loss_box_reg: 0.0002 (0.0059)  loss_objectness: 0.0016 (0.0080)  loss_rpn_box_reg: 0.4848 (0.4097)  time: 0.1983  data: 0.0040  max mem: 7464\n",
      "Epoch: [5] Total time: 0:09:11 (0.1990 s / it)\n",
      "epoch 5 loss 0.8361947536468506 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413), tensor(0.0448)]\n",
      "Epoch: [6]  [   0/2772]  eta: 0:25:26  lr: 0.000100  loss: 0.3708 (0.3708)  loss_classifier: 0.0474 (0.0474)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.0302 (0.0302)  loss_rpn_box_reg: 0.2931 (0.2931)  time: 0.5507  data: 0.3242  max mem: 7470\n",
      "Epoch: [6]  [ 500/2772]  eta: 0:07:34  lr: 0.000100  loss: 0.4444 (0.4393)  loss_classifier: 0.0461 (0.0462)  loss_box_reg: 0.0050 (0.0053)  loss_objectness: 0.0034 (0.0074)  loss_rpn_box_reg: 0.3818 (0.3803)  time: 0.1991  data: 0.0040  max mem: 7470\n",
      "Epoch: [6]  [1000/2772]  eta: 0:05:53  lr: 0.000100  loss: 0.4636 (0.4477)  loss_classifier: 0.0528 (0.0475)  loss_box_reg: 0.0055 (0.0055)  loss_objectness: 0.0064 (0.0076)  loss_rpn_box_reg: 0.3761 (0.3871)  time: 0.2006  data: 0.0042  max mem: 7470\n",
      "Epoch: [6]  [1500/2772]  eta: 0:04:13  lr: 0.000100  loss: 0.4275 (0.4500)  loss_classifier: 0.0447 (0.0476)  loss_box_reg: 0.0003 (0.0054)  loss_objectness: 0.0014 (0.0076)  loss_rpn_box_reg: 0.3504 (0.3895)  time: 0.1998  data: 0.0040  max mem: 7470\n",
      "Epoch: [6]  [2000/2772]  eta: 0:02:34  lr: 0.000100  loss: 0.4105 (0.4493)  loss_classifier: 0.0392 (0.0477)  loss_box_reg: 0.0014 (0.0054)  loss_objectness: 0.0007 (0.0074)  loss_rpn_box_reg: 0.3693 (0.3887)  time: 0.1989  data: 0.0040  max mem: 7470\n",
      "Epoch: [6]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4201 (0.4539)  loss_classifier: 0.0536 (0.0487)  loss_box_reg: 0.0047 (0.0057)  loss_objectness: 0.0026 (0.0076)  loss_rpn_box_reg: 0.3299 (0.3920)  time: 0.1982  data: 0.0040  max mem: 7470\n",
      "Epoch: [6]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.4352 (0.4523)  loss_classifier: 0.0348 (0.0487)  loss_box_reg: 0.0002 (0.0057)  loss_objectness: 0.0020 (0.0076)  loss_rpn_box_reg: 0.3380 (0.3904)  time: 0.1972  data: 0.0039  max mem: 7470\n",
      "Epoch: [6] Total time: 0:09:12 (0.1992 s / it)\n",
      "epoch 6 loss 0.9530044198036194 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413), tensor(0.0448), tensor(0.)]\n",
      "Epoch: [7]  [   0/2772]  eta: 0:23:57  lr: 0.000100  loss: 0.9369 (0.9369)  loss_classifier: 0.0385 (0.0385)  loss_box_reg: 0.0078 (0.0078)  loss_objectness: 0.0366 (0.0366)  loss_rpn_box_reg: 0.8540 (0.8540)  time: 0.5187  data: 0.3061  max mem: 7470\n",
      "Epoch: [7]  [ 500/2772]  eta: 0:07:34  lr: 0.000100  loss: 0.4602 (0.4375)  loss_classifier: 0.0392 (0.0489)  loss_box_reg: 0.0036 (0.0057)  loss_objectness: 0.0008 (0.0075)  loss_rpn_box_reg: 0.3974 (0.3754)  time: 0.2006  data: 0.0041  max mem: 7470\n",
      "Epoch: [7]  [1000/2772]  eta: 0:05:53  lr: 0.000100  loss: 0.4139 (0.4442)  loss_classifier: 0.0385 (0.0515)  loss_box_reg: 0.0048 (0.0062)  loss_objectness: 0.0018 (0.0075)  loss_rpn_box_reg: 0.3780 (0.3790)  time: 0.1983  data: 0.0041  max mem: 7470\n",
      "Epoch: [7]  [1500/2772]  eta: 0:04:13  lr: 0.000100  loss: 0.3722 (0.4399)  loss_classifier: 0.0333 (0.0500)  loss_box_reg: 0.0044 (0.0059)  loss_objectness: 0.0020 (0.0077)  loss_rpn_box_reg: 0.2948 (0.3763)  time: 0.1978  data: 0.0040  max mem: 7470\n",
      "Epoch: [7]  [2000/2772]  eta: 0:02:33  lr: 0.000100  loss: 0.3530 (0.4375)  loss_classifier: 0.0362 (0.0495)  loss_box_reg: 0.0035 (0.0060)  loss_objectness: 0.0028 (0.0083)  loss_rpn_box_reg: 0.2705 (0.3738)  time: 0.1974  data: 0.0040  max mem: 7470\n",
      "Epoch: [7]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.3802 (0.4374)  loss_classifier: 0.0393 (0.0496)  loss_box_reg: 0.0049 (0.0061)  loss_objectness: 0.0010 (0.0081)  loss_rpn_box_reg: 0.3066 (0.3737)  time: 0.1997  data: 0.0042  max mem: 7470\n",
      "Epoch: [7]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.3688 (0.4394)  loss_classifier: 0.0431 (0.0495)  loss_box_reg: 0.0028 (0.0062)  loss_objectness: 0.0011 (0.0079)  loss_rpn_box_reg: 0.3045 (0.3758)  time: 0.1985  data: 0.0041  max mem: 7470\n",
      "Epoch: [7] Total time: 0:09:12 (0.1992 s / it)\n",
      "epoch 7 loss 0.4896480143070221 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413), tensor(0.0448), tensor(0.), tensor(0.)]\n",
      "Epoch: [8]  [   0/2772]  eta: 0:27:49  lr: 0.000100  loss: 0.4288 (0.4288)  loss_classifier: 0.0757 (0.0757)  loss_box_reg: 0.0002 (0.0002)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.3494 (0.3494)  time: 0.6023  data: 0.3833  max mem: 7470\n",
      "Epoch: [8]  [ 500/2772]  eta: 0:07:35  lr: 0.000100  loss: 0.4144 (0.4252)  loss_classifier: 0.0451 (0.0489)  loss_box_reg: 0.0052 (0.0064)  loss_objectness: 0.0016 (0.0076)  loss_rpn_box_reg: 0.3245 (0.3624)  time: 0.1989  data: 0.0041  max mem: 7470\n",
      "Epoch: [8]  [1000/2772]  eta: 0:05:54  lr: 0.000100  loss: 0.3407 (0.4201)  loss_classifier: 0.0416 (0.0488)  loss_box_reg: 0.0046 (0.0063)  loss_objectness: 0.0023 (0.0072)  loss_rpn_box_reg: 0.2965 (0.3578)  time: 0.2003  data: 0.0041  max mem: 7470\n",
      "Epoch: [8]  [1500/2772]  eta: 0:04:14  lr: 0.000100  loss: 0.3961 (0.4168)  loss_classifier: 0.0325 (0.0473)  loss_box_reg: 0.0017 (0.0061)  loss_objectness: 0.0022 (0.0075)  loss_rpn_box_reg: 0.3507 (0.3559)  time: 0.1994  data: 0.0041  max mem: 7470\n",
      "Epoch: [8]  [2000/2772]  eta: 0:02:34  lr: 0.000100  loss: 0.3817 (0.4186)  loss_classifier: 0.0346 (0.0477)  loss_box_reg: 0.0017 (0.0060)  loss_objectness: 0.0034 (0.0076)  loss_rpn_box_reg: 0.3220 (0.3573)  time: 0.1969  data: 0.0039  max mem: 7470\n",
      "Epoch: [8]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4489 (0.4185)  loss_classifier: 0.0541 (0.0475)  loss_box_reg: 0.0033 (0.0061)  loss_objectness: 0.0007 (0.0075)  loss_rpn_box_reg: 0.3352 (0.3574)  time: 0.2020  data: 0.0046  max mem: 7470\n",
      "Epoch: [8]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.3416 (0.4201)  loss_classifier: 0.0340 (0.0476)  loss_box_reg: 0.0016 (0.0060)  loss_objectness: 0.0016 (0.0074)  loss_rpn_box_reg: 0.2892 (0.3591)  time: 0.1990  data: 0.0040  max mem: 7470\n",
      "Epoch: [8] Total time: 0:09:12 (0.1995 s / it)\n",
      "epoch 8 loss 0.5886488556861877 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413), tensor(0.0448), tensor(0.), tensor(0.), tensor(0.0213)]\n",
      "Epoch: [9]  [   0/2772]  eta: 0:23:26  lr: 0.000100  loss: 0.2387 (0.2387)  loss_classifier: 0.0080 (0.0080)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.0011 (0.0011)  loss_rpn_box_reg: 0.2295 (0.2295)  time: 0.5075  data: 0.3055  max mem: 7470\n",
      "Epoch: [9]  [ 500/2772]  eta: 0:07:35  lr: 0.000100  loss: 0.3415 (0.4086)  loss_classifier: 0.0384 (0.0460)  loss_box_reg: 0.0058 (0.0068)  loss_objectness: 0.0013 (0.0072)  loss_rpn_box_reg: 0.2893 (0.3486)  time: 0.2007  data: 0.0041  max mem: 7470\n",
      "Epoch: [9]  [1000/2772]  eta: 0:05:54  lr: 0.000100  loss: 0.4006 (0.4125)  loss_classifier: 0.0510 (0.0465)  loss_box_reg: 0.0039 (0.0067)  loss_objectness: 0.0036 (0.0072)  loss_rpn_box_reg: 0.3279 (0.3521)  time: 0.1985  data: 0.0041  max mem: 7470\n",
      "Epoch: [9]  [1500/2772]  eta: 0:04:14  lr: 0.000100  loss: 0.3355 (0.4164)  loss_classifier: 0.0430 (0.0480)  loss_box_reg: 0.0044 (0.0066)  loss_objectness: 0.0026 (0.0073)  loss_rpn_box_reg: 0.2832 (0.3546)  time: 0.1992  data: 0.0041  max mem: 7470\n",
      "Epoch: [9]  [2000/2772]  eta: 0:02:34  lr: 0.000100  loss: 0.4058 (0.4154)  loss_classifier: 0.0356 (0.0476)  loss_box_reg: 0.0037 (0.0063)  loss_objectness: 0.0015 (0.0072)  loss_rpn_box_reg: 0.3284 (0.3542)  time: 0.2002  data: 0.0042  max mem: 7470\n",
      "Epoch: [9]  [2500/2772]  eta: 0:00:54  lr: 0.000100  loss: 0.4107 (0.4130)  loss_classifier: 0.0397 (0.0475)  loss_box_reg: 0.0035 (0.0063)  loss_objectness: 0.0010 (0.0071)  loss_rpn_box_reg: 0.3152 (0.3521)  time: 0.2007  data: 0.0042  max mem: 7470\n",
      "Epoch: [9]  [2771/2772]  eta: 0:00:00  lr: 0.000100  loss: 0.3517 (0.4122)  loss_classifier: 0.0294 (0.0473)  loss_box_reg: 0.0039 (0.0063)  loss_objectness: 0.0009 (0.0071)  loss_rpn_box_reg: 0.3023 (0.3514)  time: 0.2006  data: 0.0041  max mem: 7470\n",
      "Epoch: [9] Total time: 0:09:13 (0.1997 s / it)\n",
      "epoch 9 loss 0.33477815985679626 total_ats [tensor(0.0394), tensor(0.), tensor(0.0118), tensor(0.), tensor(0.1413), tensor(0.0448), tensor(0.), tensor(0.), tensor(0.0213), tensor(0.1630)]\n"
     ]
    }
   ],
   "source": [
    "losses, total_ats, best_model_wts = train_val(feature_extractor, model,\n",
    "                                              train_loader, val_loader,\n",
    "                                              num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPFR4eCVfF2e",
    "outputId": "e4181be0-88b1-479e-be4c-1f6481744565"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(losses, open('/scratch/nhl256/dl_project/model/object_detection_resnet50_0504_trainlosses.pickle', \"wb\"))\n",
    "pickle.dump(total_ats, open('/scratch/nhl256/dl_project/model/object_detection_resnet40_0504_evalTotalAts.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sesiU-p20F"
   },
   "source": [
    "## Evaluate - IoU by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N7S-1srTbRg"
   },
   "outputs": [],
   "source": [
    "def prepare_pred_results(predictions):\n",
    "    pred_boxes = []\n",
    "    pred_labels = []\n",
    "    pred_scores = []\n",
    "    for prediction in predictions:\n",
    "        #print(prediction)\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "        boxes = prediction[\"boxes\"]\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        scores = prediction[\"scores\"].tolist()\n",
    "        labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "        pred_boxes.append(boxes)\n",
    "        pred_labels.append(labels)\n",
    "        pred_scores.append(scores)\n",
    "\n",
    "    return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "def reorder_coord(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((ymin, xmin, ymax, xmax), dim=1)\n",
    "\n",
    "def prepare_gt(targets):\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        boxes = reorder_coord(boxes).tolist()\n",
    "        labels = target[\"labels\"].tolist()\n",
    "        gt_boxes.append(boxes)\n",
    "        gt_labels.append(labels)\n",
    "    return gt_boxes, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2a_EldORlxqm"
   },
   "outputs": [],
   "source": [
    "# Make sure that bbox_a, bbox_b = np.array\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    #print(type(bbox_a), type(bbox_b))\n",
    "    bbox_a = np.array(bbox_a)\n",
    "    bbox_b = np.array(bbox_b)\n",
    "\n",
    "    # print(type(bbox_a), type(bbox_b))\n",
    "    # print(bbox_a.shape, bbox_b.shape)\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    # top left (i.e., ymin, xmin)\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right (i.e., ymax, xmax)\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    # Area of intersection: (tl < br) = bool, (br-tl) = (ymax-ymin) \n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahoOrX-8ZFzT"
   },
   "outputs": [],
   "source": [
    "def cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres=0.5):\n",
    "    iou_table = bbox_iou(pred_bbox_c, gt_bbox_c)\n",
    "    num_pred_bboxes = iou_table.shape[0]\n",
    "    num_gt_bboxes = iou_table.shape[1]\n",
    "    TP = np.zeros(num_pred_bboxes)\n",
    "    FP = np.zeros(num_pred_bboxes)\n",
    "    # For each pred_bounding box:\n",
    "      # Find the most relevant gt_bbox (i.e., the gt_bbox with max IoU)\n",
    "      # If IoU < threshold, then flag it as FP\n",
    "      # If IoU >= threshold, then:\n",
    "        # If that gt_bbox already has already matched with another pred_bbox:\n",
    "          # Flag it as FP\n",
    "        # Else:\n",
    "          # Flag it as TP\n",
    "\n",
    "    # TP only happens if the pred_bbox mathes with a gt_bbox\n",
    "    for i in range(num_pred_bboxes):\n",
    "        gt_bbox_index = np.argmax(iou_table[i])\n",
    "        best_pred_bbox_index_for_selected_gt_bbox = np.argmax(iou_table[:,gt_bbox_index])\n",
    "        if iou_table[i, gt_bbox_index] > iou_thres \\\n",
    "            and gt_bbox_index == best_pred_bbox_index_for_selected_gt_bbox:\n",
    "            TP[i] = 1\n",
    "        else:\n",
    "            FP[i] = 1\n",
    "\n",
    "    TP_cum = np.sum(TP)\n",
    "    FP_cum = np.sum(FP)\n",
    "\n",
    "    if (TP_cum + FP_cum) != num_pred_bboxes:\n",
    "        print(\"WRONG CALCULATION OF FP\")\n",
    "    return TP_cum, FP_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzx15kDKS3Zt"
   },
   "outputs": [],
   "source": [
    "# Test for cal_TP_FP_iou\n",
    "\n",
    "def inspect_call_TP_FP_iou(test_images, test_targets):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            # print(num_gt_bboxes)\n",
    "            # print(num_pred_bboxes)\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                print(class_TP + class_FP == num_pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8N2DCbcJe92u"
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     print('Iter {}'.format(i))\n",
    "#     test_images, test_targets = next(iter(test_loader))\n",
    "#     inspect_call_TP_FP_iou(test_images, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "honU3_7QQLwr"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(predictions, test_targets, res, iou_thres=0.5):\n",
    "\n",
    "    pred_bboxes, pred_labels, pred_scores = prepare_pred_results(predictions)\n",
    "    gt_bboxes, gt_labels = prepare_gt(test_targets)\n",
    "    # res stores the TP_FP dict for each class\n",
    "    # Each TP_FP dict stores the TP_FP for each class \n",
    "    \n",
    "    batch_total_TP = 0\n",
    "    batch_total_FP = 0\n",
    "    batch_total_FN = 0\n",
    "    batch_total_num_object = 0\n",
    "    batch_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "\n",
    "    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label in \\\n",
    "        zip(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels):\n",
    "\n",
    "        pred_bbox = np.array(pred_bbox)\n",
    "        pred_score = np.array(pred_score)\n",
    "        pred_label = np.array(pred_label)\n",
    "        gt_bbox = np.array(gt_bbox)\n",
    "        gt_label = np.array(gt_label)\n",
    "        unique_share_classes = (np.unique(np.concatenate((pred_label, gt_label))))\n",
    "        \n",
    "        for c in unique_share_classes:\n",
    "            pred_class_c_index = np.where(pred_label == c)[0]\n",
    "            pred_bbox_c = pred_bbox[pred_class_c_index]\n",
    "            gt_class_c_index = np.where(gt_label == c)[0]\n",
    "            #print(gt_class_c_index)\n",
    "            gt_bbox_c = gt_bbox[gt_class_c_index]\n",
    "            num_gt_bboxes = len(gt_class_c_index)\n",
    "            num_pred_bboxes = len(pred_class_c_index)\n",
    "            #print('class {} with {} gt_bboxes and {} pred_bboxes'.format(c, num_gt_bboxes, num_pred_bboxes))\n",
    "            if num_pred_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = 0\n",
    "                class_FN = num_gt_bboxes\n",
    "            elif num_gt_bboxes == 0:\n",
    "                class_TP = 0\n",
    "                class_FP = num_pred_bboxes\n",
    "                class_FN = 0\n",
    "            else:\n",
    "                class_TP, class_FP = cal_TP_FP_iou(pred_bbox_c, gt_bbox_c, iou_thres)\n",
    "                class_FN = num_gt_bboxes - class_TP\n",
    "                #print(class_TP + class_FP == num_pred_bboxes)\n",
    "\n",
    "            batch_total_TP += class_TP\n",
    "            batch_total_FP += class_FP\n",
    "            batch_total_FN += class_FN\n",
    "            batch_total_num_object += num_gt_bboxes\n",
    "\n",
    "            batch_res[c]['TP'] += class_TP\n",
    "            batch_res[c]['FP'] += class_FP\n",
    "            batch_res[c]['FN'] += class_FN\n",
    "\n",
    "            res[c]['TP'] += class_TP\n",
    "            res[c]['FP'] += class_FP\n",
    "            res[c]['FN'] += class_FN\n",
    "            \n",
    "    return res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nji9KKjySsLT"
   },
   "outputs": [],
   "source": [
    "# Inspect evaluate_one_batch\n",
    "def inspect_evaluate_one_batch(test_images, test_targets, final_res):\n",
    "    test_images = torch.stack(test_images)\n",
    "    #print(test_images.shape)\n",
    "    test_images = prepare_inputs(test_images)\n",
    "    #print(test_images[0].shape)\n",
    "\n",
    "    test_images = list(image.to(device) for image in test_images)\n",
    "    test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(test_images)\n",
    "\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "    return final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "qfzs9R0nS5Wr",
    "outputId": "bf329a68-8d6e-4aac-9662-391a10a96fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 16}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "batch 1\n",
      "0 20 20\n",
      "cur batch res: {0: {'TP': 0, 'FP': 0, 'FN': 1}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 19}, 3: {'TP': 0, 'FP': 0, 'FN': 0}, 4: {'TP': 0, 'FP': 0, 'FN': 0}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n",
      "final res after this batch: {0: {'TP': 0, 'FP': 0, 'FN': 2}, 1: {'TP': 0, 'FP': 0, 'FN': 0}, 2: {'TP': 0, 'FP': 0, 'FN': 35}, 3: {'TP': 0, 'FP': 0, 'FN': 2}, 4: {'TP': 0, 'FP': 0, 'FN': 1}, 5: {'TP': 0, 'FP': 0, 'FN': 0}, 6: {'TP': 0, 'FP': 0, 'FN': 0}, 7: {'TP': 0, 'FP': 0, 'FN': 0}, 8: {'TP': 0, 'FP': 0, 'FN': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "final_TP = 0\n",
    "final_FP = 0\n",
    "final_FN = 0\n",
    "final_num_objects = 0\n",
    "\n",
    "# test for 2 batches\n",
    "for i in range(2):\n",
    "    test_images, test_targets = next(iter(test_loader))\n",
    "    final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "    = inspect_evaluate_one_batch(test_images, test_targets, final_res)\n",
    "\n",
    "    print('batch {}'.format(i))\n",
    "\n",
    "    \n",
    "    print(batch_total_TP, batch_total_FN, batch_total_num_object)\n",
    "    print('cur batch res:', batch_res)\n",
    "    print('final res after this batch:', final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hf3Cii6S5gf"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, iou_thres=0.5):\n",
    "    # Evaluate. for all data point in the evaluaton set\n",
    "    final_res = {c: {'TP':0, 'FP': 0, 'FN': 0} for c in range(9)}\n",
    "    final_TP = 0\n",
    "    final_FP = 0\n",
    "    final_FN = 0\n",
    "    final_num_objects = 0\n",
    "\n",
    "    for iter_, (test_images, test_targets) in enumerate(test_loader):\n",
    "        # if iter_ % 50 == 0:\n",
    "        #     print('iter', iter_)\n",
    "        #print('iter', iter_)\n",
    "        test_images = torch.stack(test_images)\n",
    "        #print(test_images.shape)\n",
    "        test_images = prepare_inputs(test_images)\n",
    "        #print(test_images[0].shape)\n",
    "\n",
    "        test_images = list(image.to(device) for image in test_images)\n",
    "        test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "\n",
    "        model.eval()\n",
    "        predictions = model(test_images)\n",
    "\n",
    "        # Evaluate for one batch\n",
    "        final_res, batch_res, batch_total_TP, batch_total_FP, batch_total_FN, batch_total_num_object \\\n",
    "                    = evaluate_one_batch(predictions, test_targets, final_res, iou_thres=0.5)\n",
    "\n",
    "        \n",
    "        final_TP += batch_total_TP\n",
    "        final_FP += batch_total_FP\n",
    "        final_FN += batch_total_FN\n",
    "        final_num_objects += batch_total_num_object\n",
    "\n",
    "    return final_res, final_TP, final_FP, final_FN, final_num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Zop8Z75IS5ep",
    "outputId": "ed65157d-23c8-4f3f-8a91-00a27a6ee63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 50\n",
      "iter 100\n"
     ]
    }
   ],
   "source": [
    "final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFT09TgzS5cW"
   },
   "outputs": [],
   "source": [
    "def evaluate_threst_score(TP, FP, FN):\n",
    "    return (TP / (TP + FP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ejRrnjTloLBc",
    "outputId": "4c05fac2-7eb0-450a-b874-855a51e1b4e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       " 1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 2: {'FN': 3074, 'FP': 0, 'TP': 0},\n",
       " 3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       " 4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       " 5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       " 7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       " 8: {'FN': 0, 'FP': 0, 'TP': 0}}"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve5Zlqi-dvxf"
   },
   "source": [
    "## Train and Evaluate for Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_4-JrdeH59"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "epoch = 0\n",
    "print_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4fAgXgidudg"
   },
   "outputs": [],
   "source": [
    "def train_eval(model, train_loader, test_loader, iou_thres=0.5, num_epochs=10):\n",
    "    train_losses = []\n",
    "    eval_threatscores = []\n",
    "    eval_final_res = []\n",
    "    best_eval_ts = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        final_res, final_TP, final_FP, final_FN, final_num_objects = evaluate_one_epoch(test_loader, iou_thres=0.5)\n",
    "\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(final_TP, final_FP, final_FN, final_num_objects)\n",
    "        eval_final_res.append(final_res)\n",
    "        eval_ts = evaluate_threst_score(final_TP, final_FP, final_FN)\n",
    "        eval_threatscores.append(eval_ts)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"epoch: {} eval_ts {}\".format(epoch, eval_ts))\n",
    "\n",
    "        if eval_ts > best_eval_ts:\n",
    "            best_eval_ts = eval_ts \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return model, best_model_wts, train_losses, eval_final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsAuhVThduhA"
   },
   "outputs": [],
   "source": [
    "model, best_model_wts, train_losses, eval_final_res = train_eval(model, train_loader,\n",
    "                                                                 test_loader, iou_thres=0.5,\n",
    "                                                                 num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "Naw6li0b09RY",
    "outputId": "70a1d53f-67ed-4e70-b5ad-3f8377c580ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3073.0, 'FP': 12599.0, 'TP': 1.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}},\n",
       " {0: {'FN': 158, 'FP': 0, 'TP': 0},\n",
       "  1: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  2: {'FN': 3070.0, 'FP': 846.0, 'TP': 4.0},\n",
       "  3: {'FN': 390, 'FP': 0, 'TP': 0},\n",
       "  4: {'FN': 27, 'FP': 0, 'TP': 0},\n",
       "  5: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  6: {'FN': 20, 'FP': 0, 'TP': 0},\n",
       "  7: {'FN': 0, 'FP': 0, 'TP': 0},\n",
       "  8: {'FN': 0, 'FP': 0, 'TP': 0}}]"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llVJ0SeDM40i"
   },
   "source": [
    "## Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xzLLZktM4aI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7w8Da6JM8or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXslpd4-d-rQ"
   },
   "source": [
    "## Customize Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGCMc2Ppd-rR"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IC8olie21GE"
   },
   "source": [
    "#### 1. Mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "cukXGxWk1WtO",
    "outputId": "1c2db8fe-1068-4235-9348-d70fcb3ea02b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47b1e48fc071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n\u001b[1;32m      4\u001b[0m                                     aspect_ratios=((0.5, 1.0, 2.0),))\n\u001b[1;32m      5\u001b[0m roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                    num_classes=21,\n",
    "                    rpn_anchor_generator=anchor_generator,\n",
    "                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVBYaALa2xOK"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7XYTwLa26Mk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmOGwmnO26uZ"
   },
   "source": [
    "#### 2. CustomVGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIB1q2yd-rl"
   },
   "outputs": [],
   "source": [
    "def customize_VGG16():\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "    \n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "    \n",
    "    classifier = list(classifier)\n",
    "    # delete the Linear layer\n",
    "    del classifier[6]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    #freeze top4 conv layer\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "    features = nn.Sequential(*features)\n",
    "        \n",
    "    return features, classifier\n",
    "backbone, box_head = customize_VGG16()\n",
    "backbone.out_channels = 512\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                           aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pb3n-b1_pNs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "objectDectionFastRCNN_050220.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
